{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Final Project Specifications\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, we'll review all the guidelines and specifications for the final project for Module 2. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Understand all required aspects of the Final Project for Module 2\n",
    "* Understand all required deliverables\n",
    "* Understand what constitutes a successful project\n",
    "\n",
    "### Final Project Summary\n",
    "\n",
    "Another module down--you're half way there!\n",
    "\n",
    "< img src='halfway-there.gif'>\n",
    "\n",
    "For the culmination of Module 2, you just need to complete the final project!\n",
    "\n",
    "### The Project\n",
    "\n",
    "For this project, you'll be working with the Northwind database--a free, open-source dataset created by Microsoft containing data from a fictional company. You probably remember the Northwind database from our section on Advanced SQL. Here's the schema for the Northwind database:\n",
    "\n",
    "<img src='Northwind_ERD.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deliverables\n",
    "> The goal of this project is to test your ability to gather information from a real-world database and use your knowledge of statistical analysis and hypothesis testing to generate analytical insights that can be of value to the company.\n",
    "\n",
    "You will need query the database to get the data needed to perform a statistical analysis.  In this statistical analysis, **you'll need to perform a hypothesis test (or perhaps several) to answer the following question:**\n",
    "\n",
    "> **_Do discounts have a statistically significant effect on the number of products customers order? If so, at what level(s) of discount?_**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  addition to answering this question with a hypothesis test, you will also need to come up with **_at least 3 other hypotheses to test on your own_**.  These can by anything that you think could be imporant information for the company. \n",
    "[ ] Do this\n",
    "For this hypothesis, be sure to specify both the **_null hypothesis_** and the **_alternative hypothesis_** for your question.  You should also specify if this is one-tail or a two-tail test. \n",
    "\n",
    "To complete this project, you will need to turn in the following 3 deliverables:\n",
    "\n",
    "1. A **_Jupyter Notebook_** containing any code you've written for this project. \n",
    "2. A **_Blog Post_** explaining your process, methodology, and findings.  \n",
    "3. An **_\"Executive Summary\" PowerPoint Presentation_** that explains the hypothesis tests you ran, your findings, and their relevance to company stakeholders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Must-Haves\n",
    "\n",
    "For this project, your jupyter notebook should meet the following specifications:\n",
    "\n",
    "**_Organization/Code Cleanliness_**\n",
    "\n",
    "* The notebook should be well organized, easy to follow, and code is commented where appropriate.  \n",
    "<br>  \n",
    "    * Level Up: The notebook contains well-formatted, professional looking markdown cells explaining any substantial code. All functions have docstrings that act as professional-quality documentation.  \n",
    "<br>      \n",
    "* The notebook is written to technical audiences with a way to both understand your approach and reproduce your results. The target audience for this deliverable is other data scientists looking to validate your findings.  \n",
    "<br>    \n",
    "* Any SQL code written to source data should also be included.  \n",
    "\n",
    "**_Findings_**\n",
    "\n",
    "* Your notebook should clearly show how you arrived at your results for each hypothesis test, including how you calculated your p-values.   \n",
    "<br>\n",
    "* You should also include any other statistics that you find relevant to your analysis, such as effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog Post Must-Haves\n",
    "\n",
    "Your blog post should include everything from how you identified what tables contained the information you need, to how you retrieved it using SQL (and any challenges you ran into while doing so), as well as your methodology and results for your hypothesis tests. \n",
    "\n",
    "**_NOTE:_**  This blog post is your way of showcasing the work you've done on this project--chances are it will soon be read by a recruiter or hiring manager! Take the time to make sure that you craft your story well, and clearly explain your process and findings in a way that clearly shows both your technical expertise **_and_** your ability to communicate your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary Must-Haves\n",
    "\n",
    "Your presentation should:\n",
    "\n",
    "* Contain between 5-10 professional quality slides detailing:\n",
    "<br>  \n",
    "    * A high-level overview of your methodology  \n",
    "    <br>  \n",
    "    * The results of your hypothesis tests  \n",
    "    <br>  \n",
    "    * Any real-world recommendations you would like to make based on your findings (ask yourself--why should the executive team care about what you found? How can your findings help the company?)  \n",
    "    <br>  \n",
    "* Take no more than 5 minutes to present  \n",
    "<br>  \n",
    "* Avoid technical jargon and explain results in a clear, actionable way for non-technical audiences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Outline of Data Processing and Analysis<br> (using OSEMN model)\n",
    "\n",
    "1. **OBTAIN:**\n",
    "    - **Import data, inspect, check for datatypes to convert and null values**<br>\n",
    "        - Display header and info\n",
    "        - Drop any unneeded columns (df.drop(['col1','col2'],axis=1)\n",
    "\n",
    "2. **SCRUB: cast data types, identify outliers, check for multicollinearity, normalize data**<br>\n",
    "    - Check and cast data types\n",
    "        - [ ] Check for #'s that are store as objects (df.info())\n",
    "            - when converting to #'s, look for odd values (like many 0's), or strings that can't be converted\n",
    "            - Decide how to deal weird/null values (df.unique(), df.isna().sum(), df.describe()-min/max, etc\n",
    "        - [ ]  Check for categorical variables stored as integers\n",
    "    - [ ] Check for missing values  (df.isna().sum())\n",
    "        - Can drop rows or colums\n",
    "        - For missing numeric data with median or bin/convert to categorical\n",
    "        - For missing categorical data: make NaN own category OR replace with most common category\n",
    "    - [ ] Check for multicollinearity\n",
    "         - use seaborn to make correlation matrix plot [Evernote Link](https://www.evernote.com/l/AArNyaEwjA5JUL6I9PazHs_ts_hU-m7ja1I/) \n",
    "        - Good rule of thumb is anything over 0.75 corr is high, remove the variable that has the most correl with the largest # of variables\n",
    "    - [ ] Normalize data (may want to do after some exploring)\n",
    "        - Most popular is Z-scoring (but won't fix skew) \n",
    "        - Can log-transform to fix skewed data\n",
    "    \n",
    "            \n",
    "3. **EXPLORE:Check distributions, outliers, etc**\n",
    "    - [ ] Check scales, ranges (df.describe())\n",
    "    - [ ] Check histograms to get an idea of distributions (df.hist()) and dat transformations to perform\n",
    "        - Can also do kernel density estimates\n",
    "    - [ ] Use scatterplots to check for linearity and possible categorical variables (df.plot(kind-'scatter')\n",
    "        - categoricals will look like vertical lines\n",
    "    - [ ] Use pd.plotting.scatter_matrix to visualize possible relationships\n",
    "    - [ ] Check for linearity\n",
    "\n",
    "   \n",
    "4. **FIT AN INITIAL MODEL:** \n",
    "    - Various forms, detail later...\n",
    "    - **Assessing the model:**\n",
    "        - Assess parameters (slope,intercept)\n",
    "        - Check if the model explains the variation in the data (RMSE, F, R_square)\n",
    "        - *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "        - *Whats the impact of collinearity? Can we ignore?*\n",
    "5. **Revise the fitted model**\n",
    "    - Multicollinearity is big issue for lin regression and cannot fully remove it\n",
    "    - Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "    - Check for missed non-linearity\n",
    "6. **Holdout validation / Train/test split**\n",
    "    - use sklearn train_test_split \n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Import functions from project 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normal packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "# Statsmodels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check columns returns the datatype, null values and unique values of input series \n",
    "def check_column(series,nlargest='all'):\n",
    "    print(f\"Column: df['{series.name}']':\")\n",
    "    print(f\"dtype: {series.dtype}\")\n",
    "    print(f\"isna: {series.isna().sum()} out of {len(series)} - {round(series.isna().sum()/len(series)*100,3)}%\")\n",
    "        \n",
    "    print(f'\\nUnique non-na values:') #,df['waterfront'].unique())\n",
    "    if nlargest =='all':\n",
    "        print(series.value_counts())\n",
    "    else:\n",
    "        print(series.value_counts().nlargest(nlargest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# MULTIPLOT\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def multiplot(df):\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, center=0,\n",
    "                \n",
    "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SEABORN\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots histogram and scatter (vs price) side by side\n",
    "def plot_hist_scat_sns(df,target='price'):\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    \n",
    "    ## ----------- DEFINE AESTHETIC CUSTOMIZATIONS ----------- ##\n",
    "    # Axis Label fonts\n",
    "    fontTitle = {'fontsize': 16,\n",
    "               'fontweight': 'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    fontAxis = {'fontsize': 14,\n",
    "               'fontweight': 'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    fontTicks = {'fontsize': 12,\n",
    "               'fontweight':'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    # Formatting dollar sign labels\n",
    "    fmtPrice = '${x:,.0f}'\n",
    "    tickPrice = mtick.StrMethodFormatter(fmtPrice)\n",
    "    \n",
    "\n",
    "    ## ----------- PLOTTING ----------- ##\n",
    "    \n",
    "    ## Loop through dataframe to plot\n",
    "    for column in df.describe():\n",
    "    \n",
    "        # Create figure with subplots for current column\n",
    "        # Note: in order to use identical syntax for large # of subplots (ax[i,j]), \n",
    "        #  declare an extra row of subplots to be removed later\n",
    "        fig, ax = plt.subplots(figsize=(12,10), ncols=2, nrows=2)\n",
    "\n",
    "        ## ----- SUBPLOT 1 -----##\n",
    "        i,j = 0,0\n",
    "        ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "        \n",
    "        # Define graphing keyword dictionaries for distplot (Subplot 1)\n",
    "        hist_kws = {\"linewidth\": 1, \"alpha\": 1, \"color\": 'blue','edgecolor':'w'}\n",
    "        kde_kws = {\"color\": \"white\", \"linewidth\": 1, \"label\": \"KDE\"}\n",
    "        \n",
    "        # Plot distplot on ax[i,j] using hist_kws and kde_kws\n",
    "        sns.distplot(df[column], norm_hist=True, kde=True,\n",
    "                     hist_kws = hist_kws, kde_kws = kde_kws,\n",
    "                     label=column+' histogram', ax=ax[i,j])\n",
    " \n",
    "\n",
    "        # Set x axis label\n",
    "        ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "    \n",
    "        # Get x-ticks, rotate labels, and return\n",
    "        xticklab1 = ax[i,j].get_xticklabels(which = 'both')\n",
    "        ax[i,j].set_xticklabels(labels=xticklab1, fontdict=fontTicks, rotation=45)\n",
    "        ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "        \n",
    "        # Set y-label \n",
    "        ax[i,j].set_ylabel('Density',fontdict=fontAxis)\n",
    "        yticklab1=ax[i,j].get_yticklabels(which='both')\n",
    "        ax[i,j].set_yticklabels(labels=yticklab1,fontdict=fontTicks)\n",
    "        ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "        \n",
    "        \n",
    "        # Set y-grid\n",
    "        ax[i, j].set_axisbelow(True)\n",
    "        ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "        \n",
    "        ## ----- SUBPLOT 2-----  ##\n",
    "        i,j = 0,1\n",
    "        ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "        # Define the ketword dictionaries for  scatter plot and regression line (subplot 2)\n",
    "        line_kws={\"color\":\"white\",\"alpha\":0.5,\"lw\":4,\"ls\":\":\"}\n",
    "        scatter_kws={'s': 2, 'alpha': 0.5,'marker':'.','color':'blue'}\n",
    "\n",
    "        # Plot regplot on ax[i,j] using line_kws and scatter_kws\n",
    "        sns.regplot(df[column], df[target], \n",
    "                    line_kws = line_kws,\n",
    "                    scatter_kws = scatter_kws,\n",
    "                    ax=ax[i,j])\n",
    "        \n",
    "        # Set x-axis label\n",
    "        ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "         # Get x ticks, rotate labels, and return\n",
    "        xticklab2=ax[i,j].get_xticklabels(which='both')\n",
    "        ax[i,j].set_xticklabels(labels=xticklab2,fontdict=fontTicks, rotation=45)\n",
    "        ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "        # Set  y-axis label\n",
    "        ax[i,j].set_ylabel('Price',fontdict=fontAxis)\n",
    "        \n",
    "        # Get, set, and format y-axis Price labels\n",
    "        yticklab = ax[i,j].get_yticklabels()\n",
    "        ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "        ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "        # Set y-grid\n",
    "        ax[i, j].set_axisbelow(True)\n",
    "        ax[i, j].grid(axis='y',ls='--')       \n",
    "        \n",
    "        ## ---------- Final layout adjustments ----------- ##\n",
    "        # Deleted unused subplots \n",
    "        fig.delaxes(ax[1,1])\n",
    "        fig.delaxes(ax[1,0])\n",
    "\n",
    "        # Optimizing spatial layout\n",
    "        fig.tight_layout()\n",
    "        figtitle=column+'_dist_regr_plots.png'\n",
    "        plt.savefig(figtitle)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tukey's method using IQR to eliminate \n",
    "def detect_outliers(df,n,features):\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        # select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "        return multiple_outliers \n",
    "# Outliers_to_drop = detect_outliers(data,2,[\"col1\",\"col2\"])\n",
    "# df.loc[Outliers_to_drop] # Show the outliers rows\n",
    "# Drop outliers\n",
    "# data= data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
