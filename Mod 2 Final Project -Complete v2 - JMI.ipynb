{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZeLgicvwwP6"
   },
   "source": [
    "# Module 2 Final Project Submission\n",
    "- Student: James M. Irving, Ph.D.\n",
    "- Github Link: https://github.com/jirvingphd/dsc-2-final-project-online-ds-ft-021119 \n",
    "- Reviewer: Jeff Herman\n",
    "- Review Date: 04/08/19\n",
    "- Blog Post: https://jirvingphd.github.io/hypothesis_testing_with_northwind_database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXO-WQHYwwQF"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oz3uzog5wwQM"
   },
   "source": [
    "# Importing of Packages and Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: EXPORTING OF RESULTS CONTROLLED BY def export_data\n",
    "    - Set the parameter save_for_user to True if you want to export the data tables from the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def export_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: By default, the notebook will not save figures and csv files\n",
    "# Change save_for_user to True in order to save all output outputs. \n",
    "save_for_user = False \n",
    "\n",
    "\n",
    "fig_filepath = \"Figures/\"\n",
    "data_filepath = \"Exported CSV/\"\n",
    "def export_data(df_table,table_name):\n",
    "    import time \n",
    "    \n",
    "    data_filepath = \"Exported CSV/\"\n",
    "    df_table.round(4).to_csv(data_filepath+table_name+'.csv')\n",
    "    time.sleep(1)\n",
    "    df_table.round(4).to_excel(data_filepath+table_name+'.xlsx')\n",
    "    df_table.round(4).to_html(data_filepath+table_name+'.html')\n",
    "    time.sleep(1)\n",
    "#     df2png(df_table,table_name) #Commentedthis out so can run without imgkit\n",
    "    \n",
    "    \n",
    "# def export_figure(figure,figname,**kwargs):\n",
    "\n",
    "#     for k,v in kwargs:\n",
    "#         if k == 'FaceColor':\n",
    "#             FaceColor = v\n",
    "#         if k == 'dpi':\n",
    "#             dpi = v\n",
    "#         if k == 'FrameOn':\n",
    "#             FrameOn == v\n",
    "            \n",
    "#     fig_filepath = \"Figures/\"\n",
    "#     figure.savefig(fig_filepath+figname,dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbpLynD4pEbK"
   },
   "source": [
    "### Load Pandas Styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CSS table visuals\n",
    "CSS = \"\"\"\n",
    "<style type=\\\"text/css\\\">\n",
    "body {\n",
    "    margin: 0;\n",
    "    font-family: serif;\n",
    "    text-align: center;\n",
    "    font-weight: bold;\n",
    "}\n",
    "table.dataframe {\n",
    "    border-collapse: collapse;\n",
    "    border: 1px solid black;\n",
    "}\n",
    "table.dataframe tr {\n",
    "    border: 1px solid black;\n",
    "}\n",
    "table.dataframe td, table.dataframe th {\n",
    "    margin: 0.1;\n",
    "    border: 1px solid white;\n",
    "    padding-left: 0.5em;\n",
    "    padding-right: 0.5em;\n",
    "}\n",
    "table.dataframe th:not(:empty) {\n",
    "    background-color: #000000;\n",
    "    color: #ffffff;\n",
    "    text-align: center;\n",
    "    font-weight: bold;\n",
    "}\n",
    "table.dataframe tr:nth-child(2) th:empty {\n",
    "    border-left: none;\n",
    "    border-right: 1px dashed #888;\n",
    "}\n",
    "table.dataframe td {\n",
    "    border: 1px solid ##e8e8ea;\n",
    "    background-color: ##e8e8ea;\n",
    "    text-align: center; \n",
    "}\n",
    "</style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using user-defined css above\n",
    "# HTML('<style>{}</style>'.format(CSS))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Using built-in pandas styling\n",
    "pd.set_option('colheader_justify','center')\n",
    "pd.set_option('precision',3)\n",
    "\n",
    "# from IPython.core import display\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# If loading CSS from file:\n",
    "f = open('CSS.css','r')\n",
    "HTML('<style>{}</style>'.format(f.read()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pandas highlighting functions\n",
    "\n",
    "# def highlight_sig(s,alpha,column):\n",
    "#     is_sig = pd.Series(data=False, index=s.index)\n",
    "#     is_sig[column] = s.loc[column] <= threshold\n",
    "#     print(type(is_sig))\n",
    "#     return ['background-color: yellow' if is_sig.any() else '' for v in is_sig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tools to export dataframes as images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def df2png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented this out so that computers without imgkit can run the notebook. \n",
    "# # Export pandas tables to images using imgkit and wkhtmltopdf\n",
    "# import imgkit\n",
    "# import os \n",
    "# import time\n",
    "\n",
    "# imgkitoptions = {'format':'png'}\n",
    "# imgconfig = imgkit.config(wkhtmltoimage = 'C:/Users/james/Anaconda3/envs/learn-env/Lib/site-packages/wkhtmltopdf/bin/wkhtmltoimage.exe')\n",
    "\n",
    "# def df2png(df, name_for_file):\n",
    "#     '''Accepts a dataframe and a filename (without extention). Saves an image of the stylized table.'''\n",
    "#     # Now save the css and html dataframe to the same text file before conversion\n",
    "#     data = df\n",
    "#     # Note data_filepath is defined at beginning of notebook\n",
    "#     filename = data_filepath+name_for_file #no extention\n",
    "\n",
    "#     text_file_name = filename+'_to_convert.html'\n",
    "\n",
    "#     from pathlib import Path\n",
    "#     config = Path(text_file_name)\n",
    "#     if config.is_file():\n",
    "#         # Store configuration file values\n",
    "#         os.remove(text_file_name)\n",
    "\n",
    "# #     else:\n",
    "# #         # Keep presets\n",
    "\n",
    "    \n",
    "#     text_file = open(text_file_name,'a')\n",
    "#     text_file.write(CSS)\n",
    "#     text_file.write(data.to_html())\n",
    "#     text_file.close()\n",
    "\n",
    "#     imagename = filename+'.png'\n",
    "#     imgkit.from_file(text_file_name, imagename, options = imgkitoptions, config=imgconfig)\n",
    "#     time.sleep(1)\n",
    "#     os.remove(text_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVjqeNPowwQN"
   },
   "source": [
    "## Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "# Statsmodels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session, sessionmaker\n",
    "from sqlalchemy import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If running on CoLab, uncomment the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If running on googe colab\n",
    "# # #CELL A : IF ON COLAB.\n",
    "# # #The northwind.sqlite is located in: content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/', force_remount=True)\n",
    "\n",
    "# # # If in Google Drive \n",
    "# path= '/content/drive/My Drive/Colab Notebooks/datasets/'\n",
    "# file='Northwind_small.sqlite'\n",
    "# filepath = path+file\n",
    "# # filepath = '/content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite'\n",
    "# #NOTE: To save files to drive, just to df.to_csv(path+'filename.csv')\n",
    "\n",
    "# If local:\n",
    "filepath = 'Northwind_small.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6AsUbfMwwQU"
   },
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0xNoo6vwwQV"
   },
   "source": [
    "### def check_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns returns the datatype, null values and unique values of input series \n",
    "def check_column(series,nlargest='all'):\n",
    "    print(f\"Column: df['{series.name}']':\")\n",
    "    print(f\"dtype: {series.dtype}\")\n",
    "    print(f\"isna: {series.isna().sum()} out of {len(series)} - {round(series.isna().sum()/len(series)*100,3)}%\")\n",
    "        \n",
    "    print(f'\\nUnique non-na values:') #,df['waterfront'].unique())\n",
    "    if nlargest =='all':\n",
    "        print(series.value_counts())\n",
    "    else:\n",
    "        print(series.value_counts().nlargest(nlargest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qk9N1jMywwQf"
   },
   "source": [
    "### def detect_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tukey's method using IQR to eliminate \n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"Uses Tukey's method to return outer of interquartile ranges to return indices if outliers in a dataframe.\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrane containing columns of features\n",
    "    n: default is 0, multiple outlier cutoff  \n",
    "    \n",
    "    Returns:\n",
    "    Index of outliers for .loc\n",
    "    \n",
    "    Examples:\n",
    "    Outliers_to_drop = detect_outliers(data,2,[\"col1\",\"col2\"]) Returning value\n",
    "    df.loc[Outliers_to_drop] # Show the outliers rows\n",
    "    data= data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n",
    "   \"\"\"\n",
    "\n",
    "# Drop outliers    \n",
    "\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        \n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        \n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "        # select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    return multiple_outliers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDzeDK5gwwQk"
   },
   "source": [
    "### def plot_hist_scat_sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots histogram and scatter (vs price) side by side\n",
    "def plot_hist_scat_sns(df, target='index'):\n",
    "    \"\"\"Plots seaborne distplots and regplots for columns im datamframe vs target.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame.describe() columns will be used. \n",
    "    target = name of column containing target variable.assume first coluumn. \n",
    "    \n",
    "    Returns:\n",
    "    Figures for each column vs target with 2 subplots.\n",
    "   \"\"\"\n",
    "    import matplotlib.ticker as mtick\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    with plt.style.context(('dark_background')):\n",
    "        ###  DEFINE AESTHETIC CUSTOMIZATIONS  -------------------------------##\n",
    "\n",
    "\n",
    "#         plt.style.use('dark_background')\n",
    "        figsize=(9,7)\n",
    "\n",
    "        # Axis Label fonts\n",
    "        fontTitle = {'fontsize': 14,\n",
    "                   'fontweight': 'bold',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontAxis = {'fontsize': 12,\n",
    "                   'fontweight': 'medium',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontTicks = {'fontsize': 8,\n",
    "                   'fontweight':'medium',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        # Formatting dollar sign labels\n",
    "        fmtPrice = '${x:,.0f}'\n",
    "        tickPrice = mtick.StrMethodFormatter(fmtPrice)\n",
    "\n",
    "\n",
    "        ###  PLOTTING ----------------------------- ------------------------ ##\n",
    "\n",
    "        # Loop through dataframe to plot\n",
    "        for column in df.describe():\n",
    "#             print(f'\\nCurrent column: {column}')\n",
    "\n",
    "            # Create figure with subplots for current column\n",
    "            fig, ax = plt.subplots(figsize=figsize, ncols=2, nrows=2)\n",
    "\n",
    "            ##  SUBPLOT 1 --------------------------------------------------##\n",
    "            i,j = 0,0\n",
    "            ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "            # Define graphing keyword dictionaries for distplot (Subplot 1)\n",
    "            hist_kws = {\"linewidth\": 1, \"alpha\": 1, \"color\": 'blue','edgecolor':'w'}\n",
    "            kde_kws = {\"color\": \"white\", \"linewidth\": 1, \"label\": \"KDE\"}\n",
    "\n",
    "            # Plot distplot on ax[i,j] using hist_kws and kde_kws\n",
    "            sns.distplot(df[column], norm_hist=True, kde=True,\n",
    "                         hist_kws = hist_kws, kde_kws = kde_kws,\n",
    "                         label=column+' histogram', ax=ax[i,j])\n",
    "\n",
    "\n",
    "            # Set x axis label\n",
    "            ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "            # Get x-ticks, rotate labels, and return\n",
    "            xticklab1 = ax[i,j].get_xticklabels(which = 'both')\n",
    "            ax[i,j].set_xticklabels(labels=xticklab1, fontdict=fontTicks, rotation=0)\n",
    "            ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "\n",
    "            # Set y-label \n",
    "            ax[i,j].set_ylabel('Density',fontdict=fontAxis)\n",
    "            yticklab1=ax[i,j].get_yticklabels(which='both')\n",
    "            ax[i,j].set_yticklabels(labels=yticklab1,fontdict=fontTicks)\n",
    "            ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "\n",
    "            # Set y-grid\n",
    "            ax[i, j].set_axisbelow(True)\n",
    "            ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ##  SUBPLOT 2-------------------------------------------------- ##\n",
    "            i,j = 0,1\n",
    "            ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "            # Define the kwd dictionaries for scatter and regression line (subplot 2)\n",
    "            line_kws={\"color\":\"white\",\"alpha\":0.5,\"lw\":4,\"ls\":\":\"}\n",
    "            scatter_kws={'s': 2, 'alpha': 0.5,'marker':'.','color':'blue'}\n",
    "\n",
    "            # Plot regplot on ax[i,j] using line_kws and scatter_kws\n",
    "            sns.regplot(df[column], df[target], \n",
    "                        line_kws = line_kws,\n",
    "                        scatter_kws = scatter_kws,\n",
    "                        ax=ax[i,j])\n",
    "\n",
    "            # Set x-axis label\n",
    "            ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "             # Get x ticks, rotate labels, and return\n",
    "            xticklab2=ax[i,j].get_xticklabels(which='both')\n",
    "            ax[i,j].set_xticklabels(labels=xticklab2,fontdict=fontTicks, rotation=0)\n",
    "            ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "            # Set  y-axis label\n",
    "            ax[i,j].set_ylabel(target,fontdict=fontAxis)\n",
    "\n",
    "            # Get, set, and format y-axis Price labels\n",
    "            yticklab = ax[i,j].get_yticklabels()\n",
    "            ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "            ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "    #         ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "            # Set y-grid\n",
    "            ax[i, j].set_axisbelow(True)\n",
    "            ax[i, j].grid(axis='y',ls='--')       \n",
    "\n",
    "            ## ---------- Final layout adjustments ----------- ##\n",
    "            # Deleted unused subplots \n",
    "            fig.delaxes(ax[1,1])\n",
    "            fig.delaxes(ax[1,0])\n",
    "\n",
    "            # Optimizing spatial layout\n",
    "            fig.tight_layout()\n",
    "            figtitle=column+'_dist_regr_plots.png'\n",
    "#             plt.savefig(figtitle)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2exBRnep4NL_"
   },
   "source": [
    "### def plot_wide_kde_mean_sem_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wide_kde_thin_bar(series1,sname1, series2, sname2,**kwargs):\n",
    "    '''Plot series1 and series 2 on wide kde plot with small mean+sem bar plot.\n",
    "    **kwargs include:\n",
    "        title_var -- the variable that will appear in the title of both graphs. \n",
    "    '''\n",
    "    \n",
    "    ## ADDING add_gridspec usage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.stats import sem\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.ticker as ticker\n",
    "    %matplotlib inline\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    from matplotlib import rcParams\n",
    "    from matplotlib import rc\n",
    "#     mpl.rcdefaults()\n",
    "#     rcParams['font.family'] = 'serif'\n",
    "#     rcParams['hatch.color'] = 'gray'\n",
    "#     plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    mpl.rcParams['hatch.linewidth'] = 0.5\n",
    "    \n",
    "    title=series1.name\n",
    "    for k,v in kwargs.items():\n",
    "        if k.lower() == 'title_var':\n",
    "            title = v.title()\n",
    "\n",
    "    # Plot distributions of discounted vs full price groups\n",
    "#     plt.style.use('default')\n",
    "    # with plt.style.context(('tableau-colorblind10')):\n",
    "    with plt.style.context(('dark_background')):\n",
    "\n",
    "        \n",
    "\n",
    "        ## ----------- DEFINE AESTHETIC CUSTOMIZATIONS ----------- ##\n",
    "       # Axis Label fonts\n",
    "        fontSuptitle ={'fontsize': 16,\n",
    "                   'fontweight': 'bold',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontTitle = {'fontsize': 14,\n",
    "                   'fontweight': 'medium',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontAxis = {'fontsize': 12,\n",
    "                   'fontweight': 'medium',\n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "        fontTicks = {'fontsize': 10,\n",
    "                   'fontweight':'medium', \n",
    "                    'fontfamily':'serif'}\n",
    "\n",
    "\n",
    "        ## --------- CREATE FIG BASED ON GRIDSPEC --------- ##\n",
    "        \n",
    "        plt.suptitle(title+' of Units Sold', fontdict = fontSuptitle)\n",
    "\n",
    "        # Create fig object and declare figsize\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(10,5))\n",
    "        \n",
    "        \n",
    "        # Define gridspec to create grid coordinates             \n",
    "        gs = fig.add_gridspec(nrows=1,ncols=9)\n",
    "\n",
    "        # Assign grid space to ax with add_subplot\n",
    "        ax0 = fig.add_subplot(gs[0,0:7])\n",
    "        ax1 = fig.add_subplot(gs[0,7:9])\n",
    "        \n",
    "        #Combine into 1 list\n",
    "        ax = [ax0,ax1]\n",
    "        \n",
    "        ### ------------------  SUBPLOT 1  ------------------ ###\n",
    "\n",
    "        ## --------- Defining series1 and 2 for subplot 1------- ##\n",
    "        ax[0].set_title('Distribution of '+title,fontdict=fontTitle)\n",
    "\n",
    "        # Group 1: data, label, hist_kws and kde_kws\n",
    "        plotS1 = {'data': series1, 'label': sname1.title(),\n",
    "\n",
    "                   'hist_kws' :\n",
    "                    {'edgecolor': 'white', 'color':'darkgray','alpha': 1, 'lw':1},\n",
    "                    #'hatch':'//'},\n",
    "\n",
    "                   'kde_kws':\n",
    "                    {'color':'white', 'linestyle': '-', 'linewidth':2,\n",
    "                     'label':'kde'}}\n",
    "\n",
    "        # Group 2: data, label, hist_kws and kde_kws\n",
    "        plotS2 = {'data': series2,\n",
    "                    'label': sname2.title(), \n",
    "\n",
    "                    'hist_kws' :\n",
    "                    {'edgecolor': 'white','color':'lightgreen','alpha':0.8 ,'lw':1,'hatch':'//'},\n",
    "\n",
    "\n",
    "                    'kde_kws':\n",
    "                    {'color':'darkgreen','linestyle':'-','linewidth':2,'label':'kde'}}\n",
    "        \n",
    "        # plot group 1\n",
    "        sns.distplot(plotS1['data'], label=plotS1['label'],\n",
    "                   \n",
    "                     hist_kws = plotS1['hist_kws'], kde_kws = plotS1['kde_kws'],\n",
    "                     \n",
    "                     ax=ax[0])   \n",
    "      \n",
    "\n",
    "        # plot group 2\n",
    "        sns.distplot(plotS2['data'], label=plotS2['label'],\n",
    "                     \n",
    "                     hist_kws=plotS2['hist_kws'], kde_kws = plotS2['kde_kws'],\n",
    "                     \n",
    "                     ax=ax[0])\n",
    "\n",
    "        # X label\n",
    "        xlabel = series1.name\n",
    "        xl = xlabel.replace('_',' ').title()\n",
    "        ax[0].set_xlabel(xl,fontdict=fontAxis)\n",
    "        ax[0].set_ylabel('Kernel Density Estimation',fontdict=fontAxis)\n",
    "        # ticks/legend\n",
    "        ax[0].tick_params(axis='both',labelsize=fontTicks['fontsize'])   \n",
    "        ax[0].legend()\n",
    "\n",
    "        \n",
    "#         # annotating lines with means\n",
    "#         meanS1 = np.mean(plotS1['data'])\n",
    "#         meanS2 = np.mean(plotS2['data'])\n",
    "        \n",
    "#         ax[0].axvline(meanS1, linestyle='--',label=sname1)\n",
    "#         ax[0].text(meanS1,0,f'Mean:{meanS1:.2f}',rotation=90)\n",
    "    \n",
    "#         ax[0].axvline(meanS2,color='k',linestyle='--',label=sname2)\n",
    "#         ax[0].text(meanS2,0,f'Mean:{meanS2:.2f}',rotation=90)\n",
    "        \n",
    "        \n",
    "        ### ------------------  SUBPLOT 2  ------------------ ###\n",
    "        \n",
    "        # Import scipy for error bars\n",
    "        from scipy.stats import sem\n",
    "    \n",
    "        # Declare x y group labels(x) and bar heights(y)\n",
    "        x = [plotS1['label'], plotS2['label']]\n",
    "        y = [np.mean(plotS1['data']), np.mean(plotS2['data'])]\n",
    "    \n",
    "        yerr = [sem(plotS1['data']), sem(plotS2['data'])]\n",
    "        err_kws = {'ecolor':'white','capsize':4,'capthick':2,'elinewidth':2}\n",
    "\n",
    "        # Create the bar plot\n",
    "        ax[1].bar(x,y, align='edge', edgecolor='white',linewidth=2, color =[plotS1['hist_kws']['color'],plotS2['hist_kws']['color']],\n",
    "                  yerr=yerr,error_kw=err_kws, width=0.6,)\n",
    "\n",
    "        \n",
    "        # Customize subplot 2\n",
    "        ax[1].set_title('Average '+title,fontdict=fontTitle)\n",
    "        ax[1].set_ylabel('Mean +/- SEM ',fontdict=fontAxis)\n",
    "        ax[1].set_xlabel('')\n",
    "        \n",
    "        ax[1].tick_params(axis=y,labelsize=fontTicks['fontsize'])\n",
    "        ax[1].tick_params(axis=x,labelsize=fontTicks['fontsize']) \n",
    "\n",
    "        ax1=ax[1]\n",
    "        test = ax1.get_xticklabels()\n",
    "        labels = [x.get_text() for x in test]\n",
    "        ax1.set_xticklabels([plotS1['label'],plotS2['label']], rotation=45,ha='center',fontdict=fontAxis)\n",
    "        \n",
    "        print(f\"Mean of {plotS1['label']}: {np.mean(plotS1['data'])}\\n Mean of {plotS2['label']}: {np.mean(plotS2['data'])}\")\n",
    "\n",
    "#         plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig,ax\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkPB_HcdwwQo"
   },
   "source": [
    "### def list2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2df(list):#, sort_values='index'):\n",
    "    \"\"\" Take in a list where row[0] = column_names and outputs a dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    set_index -- df.set_index(set_index)\n",
    "    sortby -- df.sorted()\n",
    "    \"\"\"    \n",
    "    \n",
    "    df_list = pd.DataFrame(list[1:],columns=list[0])\n",
    "#     df_list = df_list[1:]\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekTOehxHwwQu"
   },
   "source": [
    "### def get_col_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_col_info(col_name):\n",
    "    \"\"\"Gets the column names and data types from the alchamey inspector object.\n",
    "    Returns column_info dataframe of table details.\n",
    "    \"\"\"\n",
    "    col_list = inspector.get_columns(col_name)\n",
    "    \n",
    "    column_info = [['table','column','dtype']]\n",
    "    print(f'Table Name: {col_name}\\n')\n",
    "\n",
    "    for col in col_list:\n",
    "        column_info.append([str(col_name),col['name'], col['type']])\n",
    "        \n",
    "    df = list2df(column_info)\n",
    "    return column_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7Xq2RqAR0va"
   },
   "source": [
    "### def describe_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe_outliers -- calls detect_outliers\n",
    "def describe_outliers(df):\n",
    "    \"\"\" Returns a new_df of outliers, and % outliers each col using detect_outliers.\n",
    "    \"\"\"\n",
    "    out_count = 0\n",
    "    new_df = pd.DataFrame(columns=['total_outliers', 'percent_total'])\n",
    "    for col in df.columns:\n",
    "        outies = detect_outliers(df[col])\n",
    "        out_count += len(outies) \n",
    "        new_df.loc[col] = [len(outies), round((len(outies)/len(df.index))*100, 2)]\n",
    "    new_df.loc['grand_total'] = [sum(new_df['total_outliers']), sum(new_df['percent_total'])]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrvSuzkKwwQz"
   },
   "source": [
    "### def get_full_table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_full_table_info(engine):\n",
    "    \"\"\"Gets the table names, their column namesand data types engine.\n",
    "    Returns column_info dataframe of table details.\n",
    "    \"\"\"\n",
    "    column_info = [['table','column','dtype']]\n",
    "    \n",
    "    list_tables= engine.table_names()\n",
    "    \n",
    "    for table in list_tables:\n",
    "        \n",
    "        col_list = inspector.get_columns(table)\n",
    "        \n",
    "        for col in col_list:\n",
    "            \n",
    "            column_info.append([str(table),col['name'], col['type'],col['']])\n",
    "            inspector.get_foreign_keys()\n",
    "    \n",
    "    df = list2df(column_info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDLKQNdKR4Cb"
   },
   "source": [
    "### def Cohen's d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cohen's d\n",
    "def Cohen_d(group1, group2):\n",
    "    # Compute Cohen's d.\n",
    "    # group1: Series or NumPy array\n",
    "    # group2: Series or NumPy array\n",
    "    # returns a floating point number \n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    # Calculate the pooled threshold as shown earlier\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def plot_pdfs(cohen_d=2):\n",
    "    \"\"\"Plot PDFs for distributions that differ by some number of stds.\n",
    "    \n",
    "    cohen_d: number of standard deviations between the means\n",
    "    \"\"\"\n",
    "    group1 = scipy.stats.norm(0, 1)\n",
    "    group2 = scipy.stats.norm(cohen_d, 1)\n",
    "    xs, ys = evaluate_PDF(group1)\n",
    "    pyplot.fill_between(xs, ys, label='Group1', color='#ff2289', alpha=0.7)\n",
    "\n",
    "    xs, ys = evaluate_PDF(group2)\n",
    "    pyplot.fill_between(xs, ys, label='Group2', color='#376cb0', alpha=0.7)\n",
    "    \n",
    "    o, s = overlap_superiority(group1, group2)\n",
    "    print('overlap', o)\n",
    "    print('superiority', s)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def calc_product_price, def calc_order_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of df = pd.read_sql_query(\"SELECT * FROM OrderDetail\",  engine)\n",
    "\n",
    "# Define calc_product_review to add product price column\n",
    "def calc_product_price(row):\n",
    "    '''Calculates the sub-total for the product in the row. \n",
    "    Classifies the product as OnSale or not. \n",
    "    Sub-total =  [UnitPrice*(1-Discount)*Quantity\n",
    "    \n",
    "    Returns:\n",
    "    New columns OnSale and price'''\n",
    "    \n",
    "    price = row['UnitPrice']*(1-row['Discount'])*row['Quantity']\n",
    "    row['price'] = price\n",
    "    if row['Discount']>0:\n",
    "        row['OnSale'] = 1\n",
    "    else:\n",
    "        row['OnSale'] = 0\n",
    "    return row    \n",
    "\n",
    "# Use calc_order_total to fill in order_total column\n",
    "def calc_order_total(row,df):\n",
    "    '''Takes in a specific row and the whole dataframe.\n",
    "    Groupby OrderID. Classify as discounted_order if any\n",
    "    item was on discount. \n",
    "    Returns row with added 'discounted_order' category and \n",
    "    'order_total' '''\n",
    "    order = row['OrderId']\n",
    "    df_temp = df.groupby('OrderId').get_group(order)\n",
    "\n",
    "    \n",
    "    if any(df_temp['OnSale']):\n",
    "        row['discounted_order'] = 1\n",
    "    else:\n",
    "        row['discounted_order'] = 0\n",
    "    \n",
    "    order_total = df_temp['price'].sum()\n",
    "    row['order_total'] = order_total\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_vNJ71YdowX"
   },
   "source": [
    "#### def quant_transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM DATA BEFORE RE-CALLING ORIGINAL normtest_results\n",
    "def quant_transform(data_in):\n",
    "    '''Use sklearn.preprocessing.QuantileTransformer to remove outliers from dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data_in (dict) -- dictionary with data series as values to be processed.\n",
    "    Returns: \n",
    "        data_tf (dict) -- dicionary of same keys from data_in, but transformed.\n",
    "        '''\n",
    "    \n",
    "    from sklearn import preprocessing as prep \n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "    qt =  QuantileTransformer(n_quantiles=10,output_distribution='normal')\n",
    "    data_tf = {}\n",
    "    for k,v in data_in.items():    \n",
    "        data = np.array(v).reshape(-1,1)\n",
    "        transformed = pd.Series(np.squeeze(qt.fit_transform(data)))\n",
    "      \n",
    "        data_tf[k] = transformed\n",
    "    \n",
    "    return data_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC9pxUInUYo9"
   },
   "source": [
    "## Define plotting functions for looking at each month individually \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPcTIgoDX4u-"
   },
   "source": [
    "### def make_violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting order totals per month in violin plots\n",
    "\n",
    "def make_violinplot(x,y, title=None, hue=None, ticklabels=None):\n",
    "    '''Plots a violin plot with horizontal mean line, inner stick lines'''\n",
    "\n",
    "    plt.style.use('dark_background')\n",
    "    fig,ax =plt.subplots(figsize=(12,10))\n",
    "\n",
    "    ax.xlabel= x.name\n",
    "    ylabel= y.name\n",
    "    \n",
    "    sns.violinplot(x, y,cut=2,split=True, scale='count', scale_hue=True,\n",
    "                 saturation=.5, alpha=.9,bw=.25, palette='Dark2',inner='stick', hue=hue).set_title(title)\n",
    "\n",
    "    ax.axhline(y.mean(),label='total mean', ls=':', alpha=.5, color='xkcd:yellow')\n",
    "    ax.set_xticklabels(ticklabels)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#     x= df_year_orders['month']\n",
    "#     y= df_year_orders['order_total']\n",
    "#     title = 'Order totals per month with or without discounts'\n",
    "#     hue=df_year_orders['Discount']>0\n",
    "    \n",
    "    return fig, ax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZXCOjzFbspu"
   },
   "source": [
    "### def make-stripplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stripplot(x, y, title=None, hue=None, ticklabels=None):\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        fig,ax =plt.subplots(figsize=(8,6))\n",
    "\n",
    "\n",
    "        sns.stripplot(x, y, jitter=True, size=12,edgecolor='gray',linewidth=1.5, alpha=.5, palette='Dark2',marker='d', hue=hue).set_title(title)\n",
    "\n",
    "        ax.axhline(y.mean(),label='total mean', ls=':', alpha=.5, color='xkcd:yellow')\n",
    "        ax.set_xticklabels(ticklabels)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBrj92x2Us3C"
   },
   "source": [
    "### def draw_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "def draw_histograms(df, variable, sample_dict, n_rows, n_cols):\n",
    "\n",
    "  '''Takes dataframe, variable is column name , plots histograms '''\n",
    "  \n",
    "  with plt.style.context('seaborn-paper'):\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    for k,v in sample_dict.items():\n",
    "\n",
    "      month = df[df[variable] == k]['order_total']\n",
    "      month_mean = round(np.mean(month),2)\n",
    "\n",
    "      year =  df[df['month'] != k]['order_total']\n",
    "      year_mean = round(np.mean(year),2)\n",
    "\n",
    "      ax = fig.add_subplot(n_rows,n_cols,k)\n",
    "      ax.tick_params(labelsize=8)\n",
    "\n",
    "      plt.hist(year, bins=90,alpha=.7, label='Rest of Year')\n",
    "      plt.hist(month, alpha=.6,label= v.title())\n",
    "\n",
    "      ax.set_title(v.title(),fontsize=14)\n",
    "\n",
    "      plt.axvline(month_mean, color='xkcd:fuchsia',linestyle='--',\n",
    "                  label='Sample Mean \\n'+str(month_mean))\n",
    "\n",
    "      plt.axvline(year_mean,color='xkcd:green',linestyle='-',\n",
    "                  label='Pop. Mean \\n'+str(year_mean))\n",
    "\n",
    "      plt.legend(fontsize=6, frameon=False, ncol = 2 )\n",
    "\n",
    "    fig.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR6ajM4YVH3z"
   },
   "source": [
    "### def draw_histograms_sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histograms_from_sample(population,sample, sample_dict, n_rows, n_cols):\n",
    "\n",
    "  fig = plt.figure(figsize=(8.5,7.5))\n",
    "  count = 0\n",
    "  \n",
    "  for k,v in sample_dict.items():\n",
    "    \n",
    "    count += 1                        \n",
    "\n",
    "    month = sample_dict[k] #pop_samp_month_dict[k]\n",
    "    month_mean = round(np.mean(v),2)\n",
    "    \n",
    "    year = population\n",
    "    year_mean = round(np.mean(population),2)\n",
    "\n",
    "    ax = fig.add_subplot(n_rows,n_cols, count)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "    plt.hist(year, alpha=.8, label='All Months')\n",
    "    plt.hist(month, alpha=.6, label = v.title())\n",
    "\n",
    "    ax.set_title(k.title(),fontsize=14)\n",
    "\n",
    "    plt.axvline(month_mean, color='xkcd:fuchsia',linestyle='--',\n",
    "                label='Sample Mean \\n'+str(month_mean))\n",
    "    plt.axvline(year_mean,color='xkcd:green',linestyle='-',\n",
    "                label='Pop. Mean \\n'+str(year_mean))\n",
    "    \n",
    "    plt.legend(fontsize=6, frameon=False)\n",
    "    \n",
    "  fig.tight_layout()    \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def tukey_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tukey_table(tukey_results):\n",
    "    ''' Accepts the output of the tukey hsd test and returns it as a dataframe.'''\n",
    "    table_tukey = pd.DataFrame(data=tukey_results._results_table.data[1:], columns=tukey_results._results_table.data[0])\n",
    "    table_tukey['group1'] = table_tukey['group1'].astype('str')\n",
    "    table_tukey['group2'] = table_tukey['group2'].astype('str')\n",
    "    # Turn column names into title-capitalized \n",
    "    table_tukey.columns = [str(x).title() for x in table_tukey.columns]\n",
    "    return table_tukey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normtest_results(dict_data):\n",
    "    \"\"\"Peforms both d'agostino-pearson and shapiro-wilik normal tests\n",
    "        \n",
    "        Parameters:\n",
    "            dict_data -- dictionary with {'name' : data} \n",
    "        Returns:\n",
    "            results_normtest -- list of test results (can run list2df(results_normtest)\n",
    "    \"\"\"\n",
    "    from numpy.random import seed\n",
    "    from numpy.random import randn\n",
    "    from scipy.stats import shapiro\n",
    "    from scipy.stats import normaltest\n",
    "\n",
    "    results_normtest_shap = [['DataIn','Test','stat','p']]\n",
    "    results_normtest_dagp = [['DataIn','Test','stat','p']]\n",
    "\n",
    "    for key,val in dict_data.items():\n",
    "\n",
    "        data_in = val\n",
    "        name = key\n",
    "        test = 'Shapiro'\n",
    "        stat, p = shapiro(data_in)\n",
    "        results_normtest_shap.append([name , test, stat , p ])\n",
    "        test = 'D’Agostino’s'\n",
    "        stat, p = normaltest(data_in)\n",
    "        results_normtest_dagp.append([name,test,stat, p])\n",
    "\n",
    "    results_normtest = pd.concat([list2df(results_normtest_shap), list2df(results_normtest_dagp)]) \n",
    "\n",
    "    return results_normtest #, list2df(results_normtest_shap),list2df(results_normtest_dagp)\n",
    "\n",
    "#   results_pivot = results.pivot(index='DataIn', columns= 'Test')\n",
    "#   results_pivot.stack(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnE1XlB9eJ8d"
   },
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL DESIGN AND HYPOTHESES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rzJx4u8wwQL"
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "\n",
    "You will need query the database to get the data needed to perform a statistical analysis.  In this statistical analysis, **you'll need to perform a hypothesis test (or perhaps several) to answer the following question:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Hypothesis 1:\n",
    "> **Do discounts have a statistically significant effect on the number of products customers order?**\n",
    "> **If so, at what level(s) of discount?**\n",
    "\n",
    "\n",
    "- $H_1$ : Products that are discounted sell in higher quantities.\n",
    "\n",
    "- $H_0$ : Products that are discounted sell the same quantities as full-proce products.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Hypothesis 2:\n",
    "> **Do customers spend more money if they are buying discounted items?**\n",
    "\n",
    "- $H_1$: Customers spend more money overall when their order includes discounted items.\n",
    "\n",
    "- $H_0$: Customers spend the same amount regardless of discounted items. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Hypothesis 3:\n",
    "\n",
    "> **Does the time of year affect quantity of items sold?**\n",
    "\n",
    "- $H_1$ = THe month an order is placed relates to either a higher or lower mean quantity of items sold.\n",
    "- $H_0$ = The month of an order has no affect on the mean quantity of items sold.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Hypothesis 4:\n",
    "> **Do different countries respond to discounts differently?**\n",
    "\n",
    "- $H_1$: Different countries purchase different quantities of discounted vs non discounted products. \n",
    "- $H_0$: All countries purchase the same quantities of discounted vs non discounted products. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcYXKTJe-bjt"
   },
   "source": [
    "___\n",
    "# **Hypothesis 1:**\n",
    "> **Do discounts have a statistically significant effect on the number of products customers order?**\n",
    "> **If so, at what level(s) of discount?**\n",
    "\n",
    "\n",
    "- $H_1$ : Products that are discounted sell in higher quantities.\n",
    "\n",
    "- $H_0$ : Products that are discounted sell the same quantities as full-proce products.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Specific Aims:**\n",
    "\n",
    "* ***Aim 1:To select the proper dataset for analysis, perform EDA, and generate data groups for testing.***\n",
    "    - Used sqlalchemy and pandas.<br>\n",
    "    ```python \n",
    "    query = \"SELECT* FROM OrderDetails, GROUPBY discount\"\n",
    "    python pandas.read_sql_query()\n",
    "    ```\n",
    "* ***Aim 2: Select the appropriate t-test based on tests for the assumptions of normality and homogeneity of variance.***\n",
    "    1. **Test for Normality**\n",
    "        - D'Agostino-Pearson's normality test<br>\n",
    "        ```scipy.stats.normaltest```\n",
    "        - Shapiro-Wilik Test<br>\n",
    "        ```scipy.stats.shapiro```\n",
    "    2. **Test for Homogeneity of Variance**\n",
    "        - Levene's Test<br>\n",
    "         ```scipy.stats.levene```\n",
    "\n",
    "    3. **Choose appropriate test based upon 1. and 2.** \n",
    "        - Mann-Whitney U test (non parametric equivalent of ANOVA)<br>\n",
    "        ```stats.mannwhitneyu```<br>\n",
    "        \n",
    "        <br>\n",
    "* ***Aim 3: To perform post-hoc painrwise comparison testing to determine which level of discounts affect quantity and if any discount has a greater effect than the others.***\n",
    "     - Tukey's test for multiple pairwise comparisons<br>\n",
    "     ```statsmodels.stats.multicomp.pairwise_tukeyhsd```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mKcl3fkwwQ6"
   },
   "source": [
    "_________\n",
    "## ***H1, Aim 1.1 : To select the proper dataset for analyiss  and generate data groups for testing.***\n",
    "\n",
    "#### Importing Method\n",
    "- Used sqlalchemy to create engine to connect to Northwind_small.sqlite.\n",
    "\n",
    "- Used ```pd.read_sql_query('SELECT * FROM OrderDetail',engine)``` to directly read db into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #CELL A : IF ON COLAB.\n",
    "# # #The northwind.sqlite is located in: content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/', force_remount=True)\n",
    "\n",
    "# # # If in Google Drive \n",
    "# path= '/content/drive/My Drive/Colab Notebooks/datasets/'\n",
    "# file='Northwind_small.sqlite'\n",
    "# filepath = path+file\n",
    "# # filepath = '/content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite'\n",
    "\n",
    "\n",
    "# #NOTE: To save files to drive, just to df.to_csv(path+'filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CELL B: If RUNNING LOCALLY.\n",
    "# filepath = 'Northwind_small.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zP8k3PvuBRmw"
   },
   "source": [
    "#### Use sqlalchemy and pandas to inspect data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for sql\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "# from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "filepath =  'Northwind_small.sqlite'\n",
    "engine = create_engine('sqlite:///Northwind_small.sqlite',echo=True);\n",
    "\n",
    "inspector = inspect(engine);\n",
    "\n",
    "db_tables = inspector.get_table_names();\n",
    "\n",
    "print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list2df(get_col_info('OrderDetail'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gT_dk3tLBRm2"
   },
   "source": [
    "#### Load in table OrderDetail for hypothesis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to test hypothesis one, so we need OrderDetail table.\n",
    "table_to_test = \"OrderDetail\"\n",
    "df_od = pd.read_sql_query(\"SELECT * FROM OrderDetail\",  engine)\n",
    "df_o = pd.read_sql_query(\"SELECT * FROM [Order]\",  engine)\n",
    "# df_o.info()\n",
    "df_o.rename({'Id':'OrderId'},axis=1,inplace=True)\n",
    "\n",
    "df = df_od.merge(df_o, on = 'OrderId')\n",
    "\n",
    "# if save_for_user==True:\n",
    "#     df.to_csv(data_filepath+'OrderId_Order.csv')\n",
    "# # df.to_csv('OrderId_Order.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the levels of discount\n",
    "# sorted(df['Discount'].unique())\n",
    "counts = df['Discount'].value_counts()\n",
    "counts.sort_index()\n",
    "count_dict = dict(zip(counts.index, counts.values))\n",
    "# count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFvh-eMWBRm9"
   },
   "source": [
    "### Aim 1.2: EDA on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Quantity'].groupby(df['Discount']).plot(kind='bar')\n",
    "plot_hist_scat_sns(df.drop(['OrderId','ProductId'],axis=1),'Quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1V-i0y6wwTB",
    "scrolled": true
   },
   "source": [
    "#### Note on OrderDetails df:\n",
    "1. There are 11 possible values for discounts: \n",
    "    - Values = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1, 0.15, 0.2, 0.25]<br>\n",
    "    - However, there are very few entries for some of the levels of value.<br>\n",
    "    ``` print(sorted(df['Discount'].unique())) ```\n",
    "2. The data for quantity looks skewed and not-normal. \n",
    "    - Normality tests will likely come back significant \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrgngwVVBRnB"
   },
   "source": [
    "### Aim 1.3: Generated and inspect the required groups\n",
    "- Generate the groups that we want to test.\n",
    "    - Discounted\n",
    "    - Full price \n",
    "- Once review, use info to determine next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the descriptive statistics.\n",
    "df_discounted = df.loc[df['Discount']>0]\n",
    "df_fullprice = df.loc[df['Discount']==0]\n",
    "\n",
    "print(f'\\nAll Quantities:')\n",
    "# df.drop(['OrderId','ProductId'],axis=1).describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'\\nDiscounted Quantities:')\n",
    "\n",
    "# df_discounted.drop(['OrderId','ProductId'],axis=1).describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Full price Quantities:\\n')\n",
    "# df_fullprice.drop(['OrderId','ProductId'],axis=1).describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53olqpmbBRnI"
   },
   "source": [
    "### Plotting Summary EDA Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'title_var':'Quantity Sold'}\n",
    "figa,ax = plot_wide_kde_thin_bar(df_fullprice['Quantity'],'Full Price',df_discounted['Quantity'],'Discounted',**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this cell to save the outputs.\n",
    "# figa.savefig('H1_Kde_bar_Quantity_black.png',dpi=300,facecolor='black')\n",
    "# df_discounted.to_csv('df_discounted.csv')\n",
    "# df_fullprice.to_csv('df_fullprice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fullprice.info(), df_discounted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-uvY3EMwwRg"
   },
   "source": [
    "## ***H1, Aim 2: Select the appropriate t-test based on tests for the assumptions of normality and homogeneity of variance.***\n",
    "1. **Test for Normality**\n",
    "    - D'Agostino-Pearson's normality test<br>\n",
    "    ```scipy.stats.normaltest```\n",
    "    - Shapiro-Wilik Test<br>\n",
    "    ```scipy.stats.shapiro```\n",
    "2. **Test for Homogeneity of Variance**\n",
    "    - Levene's Test<br>\n",
    "     ```scipy.stats.levene```\n",
    "\n",
    "3. **Choose appropriate test based upon 1. and 2.** \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oo7In-gvtezW"
   },
   "source": [
    "___\n",
    "## ***H1, Aim 2: Final Workflow for testing assumptions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARClmsCLLoNf"
   },
   "source": [
    "#### New Statistical Analysis Pipeline\n",
    "1. **Test for Normality**\n",
    "    - D'Agostino-Pearson's normality test<br>\n",
    "    ```scipy.stats.normaltest```\n",
    "    - Shapiro-Wilik Test<br>\n",
    "    ```scipy.stats.shapiro```<br>\n",
    "    \n",
    "2. **Test for Homogeneity of Variance**\n",
    "    - Levene's Test<br>\n",
    "    ```scipy.stats.levene```\n",
    "\n",
    "3. **Choose appropriate test based upon 1. and 2.** <br> \n",
    "    - Welch's T-Test\n",
    "    - Mann Whitney U\n",
    "    - ANOVA \n",
    "    - Tukey's\n",
    "    \n",
    "4. Calculate effect size for significant results. \n",
    "    - Effect size: [cohen's d](https://stackoverflow.com/questions/21532471/how-to-calculate-cohens-d-in-python)\n",
    "    - Interpretation:\n",
    "        - Small effect = 0.2 ( cannot be seen by naked eye)\n",
    "        - Medium effect  = 0.5\n",
    "        - Large Effect = 0.8 (can be seen by naked eye)\n",
    "        \n",
    "5. If significant, follow up with post-hoc tests (if have more than 2 groups)\n",
    "    - [Tukey's](https://www.statsmodels.org/stable/generated/statsmodels.stats.multicomp.pairwise_tukeyhsd.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Separating groups from dfH (dataframeHypothesis)\n",
    "dfH = df[['Quantity','UnitPrice','Discount']].copy()\n",
    "\n",
    "#Adding group definitions\n",
    "grpA = \"['Discount'] > 0\"\n",
    "grpB = \"['Discount'] == 0\"\n",
    "\n",
    "# Using eval to create df_A and df_B; will allow us to fill in our results \n",
    "# with the exact definitions of Group A and Group B\n",
    "df_A = eval(f'dfH.loc[dfH{grpA}]')\n",
    "df_B = eval(f'dfH.loc[dfH{grpB}]')\n",
    "\n",
    "# dfj_D = dfH.loc[dfj['Discount']>0] \n",
    "# dfj_F = dfH.loc[dfj['Discount']==0]\n",
    "df_A.describe(),'--'*25,df_B.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VAimpHrBRoL"
   },
   "source": [
    "### Aim 2.1: Test for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding table to collect summary statistics\n",
    "H1_tests = [['Group','TestName',\n",
    "             'Test Purpose','stat',\n",
    "             'p','p<.05?'] ]\n",
    "from scipy.stats import normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1_tests = [['Group','TestName','Test Purpose','stat','p']  ]\n",
    "# 2. Testing normality\n",
    "\n",
    "test_purpose = 'Normality'\n",
    "test_to_run = 'normaltest'\n",
    "\n",
    "arrA = np.array(df_A['Quantity'])\n",
    "arrB = np.array(df_B['Quantity'])\n",
    "\n",
    "statA, pA = eval(test_to_run)(arrA)\n",
    "statB, pB = eval(test_to_run)(arrB)\n",
    "\n",
    "print(f'A:stat={statA}, p={pA}')\n",
    "print(f'B:stat={statB}, p={pB}')\n",
    "\n",
    "# H1_tests = [['Group','TestName','Test Purpose','stat','p']  ]\n",
    "H1_tests.append(['A: '+ grpA, test_to_run,\n",
    "                 test_purpose, statA,pA, pA <.05])\n",
    "\n",
    "H1_tests.append(['B: '+ grpB, test_to_run,\n",
    "                 test_purpose, statB,pB, pB <.05])\n",
    "# list2df(H1_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H1_tests).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpFPGfa1BRoQ"
   },
   "source": [
    "### Aim 2.2 Test for Homogeneity of Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test for homo var\n",
    "test_to_run = 'stats.levene'\n",
    "stat,p = eval(test_to_run)(arrA, arrB, center='median')\n",
    "\n",
    "H1_tests.append(['A&B',test_to_run,'Equal Var',stat,p, p<.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0OlDfLLBRoT"
   },
   "source": [
    "### Aim 2.3 Selecting the appropriate test to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H1_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zq38mtURQaoN"
   },
   "source": [
    "### Conclusions so far...\n",
    "- We failed both normaltests and homo variance\n",
    "- Need non-parametric 2sample ttest\n",
    "    - [**Mann-Whitney U test**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann Whitney U test+\n",
    "import scipy.stats as stats\n",
    "# arrA was discounted, arrB was non-discounted\n",
    "test_to_run = 'stats.mannwhitneyu'\n",
    "stat, p = eval(test_to_run)(arrA,arrB,alternative='greater')\n",
    "stat, p\n",
    "H1_tests.append(['A vs. B',test_to_run, 'H1_sig', stat,p, p<.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H1_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhPYorQYBRof"
   },
   "source": [
    "#### Calculating Effect Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Cohens d\n",
    "d = Cohen_d(arrA,arrB)\n",
    "print(f\"Cohen's d={round(d,3)}\")\n",
    "\n",
    "# Append H1_tests and export result. \n",
    "H1_tests.append(['A vs. B',\"Cohen's d\",'Effect Size',d,'--','--'])\n",
    "table_H1_test_results = list2df(H1_tests)\n",
    "\n",
    "# Export csv if user sets save_for_user to True\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H1_tests),'table_H1_test_results')\n",
    "#     table_H1_test_results.to_csv(data_filepath+'table_H1_test_results.csv')\n",
    "#     table_H1_test_results.to_excel(data_filepath+'table_H1_test_results.xlsx')\n",
    "#     table_H1_test_results.round(4).to_html(data_filepath+'table_H1_test_results.html')\n",
    "\n",
    "\n",
    "# Display output                          \n",
    "table_H1_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table of basic info on central tendency\n",
    "from scipy.stats import sem\n",
    "H1_mean_sem = [['H#','Group','mean','sem','std','n']]\n",
    "\n",
    "dataArr=arrA\n",
    "H1_mean_sem.append(['H1',f'A: {grpA}',np.mean(dataArr),\n",
    "                    sem(dataArr), np.std(dataArr),len(dataArr)])\n",
    "\n",
    "dataArr=arrB\n",
    "H1_mean_sem.append(['H1',f'B: {grpB}',np.mean(dataArr),\n",
    "                    sem(dataArr), np.std(dataArr),len(dataArr)])\n",
    "\n",
    "table_H1_mean_sem = list2df(H1_mean_sem)\n",
    "\n",
    "# Export csv if user sets save_for_user to True\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H1_mean_sem),'table_H1_mean_sem')\n",
    "#     table_H1_mean_sem.to_csv(data_filepath+'table_H1_mean_sem.csv')    \n",
    "#     table_H1_mean_sem.to_excel(data_filepath+'table_H1_mean_sem.xlsx')    \n",
    "#     table_H1_mean_sem.round(2).to_html(data_filepath+'table_H1_mean_sem.html')    \n",
    "\n",
    "table_H1_mean_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHEx112CTia4"
   },
   "source": [
    "#### Interpretation of results. \n",
    "**We have a significant Mann Whitney U results, but we have yet to say anything about**\n",
    "- We should now run post-hoc tests using pairwise Tukey's tests \n",
    "- Compare all levels of discount in pairwise comparisons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tlxz4CgRBRoj"
   },
   "source": [
    "## ***H1, Aim 3: To perform post-hoc pairwise comparisons for level of discount***\n",
    "- **To find out which level of discount is most effective/significant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df['Discount'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNKUUYlR7WDu"
   },
   "source": [
    "### Creating Discount Level Variables for Tukey\n",
    "- Adding LabelEncoding\n",
    "- Question: do we want to have \"None\" encoded? Not sure. Would need to replace the current np.NaN with \"NaN\" first or likely will return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervalIndex = pd.IntervalIndex(())\n",
    "df_cut = df.copy().drop('Discount',axis=1)\n",
    "\n",
    "new_discounts = pd.cut(df['Discount'],[-0.05,0.0,\n",
    "                                       0.05,0.10,\n",
    "                                       0.15,0.20,\n",
    "                                       0.25,0.3], include_lowest=False)\n",
    "new_discouts = new_discounts.cat.as_ordered(inplace=True)\n",
    "df_disc_codes  = new_discounts.cat.codes\n",
    "\n",
    "# Save code info for later\n",
    "disc_intervals = new_discounts.unique().sort_values()\n",
    "disc_codes = sorted(df_disc_codes.unique())\n",
    "\n",
    "# Save the intervals \n",
    "labelCodes = tuple(zip(disc_intervals,disc_codes))\n",
    "labelCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before encoding\n",
    "df_H1codes = pd.concat([df_cut,new_discounts,df_disc_codes],axis=1)\n",
    "# df_H1codes.info()\n",
    "# df_H1codes.Discount.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colnames = list(df_H1codes.columns)\n",
    "# colnames[-1] = 'DiscSize'\n",
    "# df_H1codes.columns = colnames\n",
    "# df_H1codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq0W7aFQdt1O"
   },
   "source": [
    "### Use Tukey's Pairwise Multiple Comparison test.\n",
    "```statsmodels.stats.multicomp.pairwise_tukeyhsd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tukey's test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd as tukey\n",
    "int_str = [str(x) for x in new_discounts]\n",
    "\n",
    "# Run tukey's test\n",
    "tukey_results =tukey(df_H1codes['Quantity'], int_str, 0.05)\n",
    "\n",
    "table_H1_tukey = tukey_table(tukey_results)\n",
    "\n",
    "if save_for_user==True:\n",
    "    export_data(table_H1_tukey,'table_H1_tukey_full')\n",
    "    export_data(table_H1_tukey.loc[table_H1_tukey['Reject']==True], 'table_H1_tukey_sig_only')\n",
    "\n",
    "#     table_H1_tukey.to_csv(data_filepath+'table_H1_tukey.csv')\n",
    "#     table_H1_tukey.to_csv(data_filepath+'table_H1_tukey.xlsx')\n",
    "#     table_H1_tukey.round(4).to_html(data_filepath+'table_H1_tukey.html')\n",
    "tukey_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_H1_tukey.loc[table_H1_tukey['Reject']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting summary post-hoc figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "H1_means = df_H1codes.groupby('Discount')['Quantity'].mean()\n",
    "H1_sem = df_H1codes.groupby('Discount')['Quantity'].sem()\n",
    "H1_std = df_H1codes.groupby('Discount')['Quantity'].std()\n",
    "H1_n =  df_H1codes.groupby('Discount')['Quantity'].count()\n",
    "H1_index = H1_means.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1_groups_mean_sem={'group':H1_index,'mean': H1_means,'sem':H1_sem,'n':H1_n}\n",
    "\n",
    "table_H1_groups_mean_sem=pd.DataFrame.from_dict(H1_groups_mean_sem, orient='columns')\n",
    "table_H1_groups_mean_sem\n",
    "\n",
    "if save_for_user == True:\n",
    "    export_data(table_H1_groups_mean_sem,'table_H1_groups_mean_sem')\n",
    "#     table_H1_groups_mean_sem.to_excel(data_filepath+'table_H1_groups_mean_sem.xlsx')\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1_means.index\n",
    "xticklabels = ['0 %','<=5 %',\n",
    "               '5-10 %','10-15 %',\n",
    "               '15-20 %','20-25 %',\n",
    "               '25-30 %']\n",
    "plot_data = {'index':H1_index,'labels':xticklabels,\n",
    "             'mean':H1_means,'sem':H1_sem,'std':H1_std}\n",
    "\n",
    "df_plot = pd.DataFrame.from_dict(plot_data,orient='columns')\n",
    "df_plot.info(), df_plot.set_index('labels',inplace=True)\n",
    "\n",
    "df_plot.to_excel(data_filepath+'Discount Level Barplot mean sem.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "with plt.style.context('dark_background'):\n",
    "#     sns.set_palette('viridis')\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    color = ['#2F9AE1', '#90E13A', '#4F3C8E', '#E95969', '#FEDF08']\n",
    "    \n",
    "    err_kws = {'ecolor': 'w', 'capsize':5,\n",
    "               'elinewidth': 2 ,'capthick': 2}\n",
    "    \n",
    "    fontTitle = {'fontfamily':'serif',\n",
    "               'fontweight':'medium',\n",
    "               'fontsize':16}\n",
    "    \n",
    "    fontAxis = {'fontfamily':'serif',\n",
    "               'fontweight':'medium',\n",
    "               'fontsize':12}\n",
    "    \n",
    "    ax.bar(x = df_plot.index,width=0.6, height = df_plot['mean'], yerr = df_plot['sem'], error_kw=err_kws,\n",
    "          edgecolor='white', linewidth = 2,color= color, alpha=.7)\n",
    "    \n",
    "    ax.set_title('Quantities Purchased By Discount Size',fontdict=fontTitle)\n",
    "    ax.set_ylabel('Average Quantity',fontdict=fontAxis)\n",
    "    ax.set_xlabel('Discount Size', fontdict=fontAxis)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure of save_for_user is True\n",
    "if save_for_user == True:\n",
    "    # ## Uncomment this cell in order to save the image.*\n",
    "#     export_figure(fig,'H1_Avg_quantity_by_discount.png')\n",
    "    fig.savefig(fig_filepath+'H1_Avg_quantity_by_discount.png', dpi=300, frameon=True,facecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Conclusions for Hypothesis 1:***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table_H1_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JKcx3pw5_LK"
   },
   "source": [
    "1. **We first used D'Agostino & Perason's test (normaltest) to check for normal distributions**\n",
    "    -  Both groups had p-values very far below our $\\alpha=0.05$  (Rows 0,1 below)\n",
    "        - We rejecteded the null hypothesis that the groups came from a populatiin with a normal distribtuion. \n",
    "2. **We tested for equal variances using Levene's test.**\n",
    "    - Level's test had a signifcant p-value, so we rejected the hypothesis that the groups have equal variances\n",
    "    \n",
    "3. **We then concluded we need a non-parametric 2-sample test, so we used the Mann-Whitney U test**. \n",
    "    -  Our comparison had a p-value less than .05\n",
    "    - We reject the null hypothesis that discounts do not affect quantities sold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qcn4RlASByQm"
   },
   "source": [
    "4. **To determine which level of discounts had the largest effect, we performed a pairwise multiple comparison Tukey's test**. \n",
    "    - Our results showed that discounts of 0-5%, 10-15%, 15-20%, and 20-25% were all significantly different from full price products. \n",
    "        - Except 5-10 % discount group\n",
    "    - No discount groups were significantly different than other discount groups.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results into a dataframe\n",
    "table_H1_tukey.loc[table_H1_tukey['Reject']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_H1codes.info(),\n",
    "# dfH.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save results to new dataframe\n",
    "# df_orig_save = df_H1codes.copy()\n",
    "# dfH1_save = dfH.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P23rLwIWwwTC"
   },
   "source": [
    "______\n",
    "# **Hypothesis 2:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeQIJm_ESU6C"
   },
   "source": [
    "> **Do customers spend more money if they are buying discounted items?**\n",
    "\n",
    "- $H_1$: Customers spend more money overall when their order includes discounted items.\n",
    "\n",
    "- $H_0$: Customers spend the same amount regardless of discounted items. \n",
    "\n",
    "**Specific Aims:**\n",
    "\n",
    "* ***Aim 1:To select the proper dataset for analysis, perform EDA, and generate data groups for testing.***\n",
    "    - Used sqlalchemy and pandas.read_sql_query()\n",
    "    \n",
    "    ```python \n",
    "query = \"SELECT* FROM OrderDetails,\n",
    "            GROUPBY discount```\n",
    "\n",
    "* ***Aim 2: Select the appropriate t-test based on tests for the assumptions of normality and homogeneity of variance.***\n",
    "    1. **Test for Normality**\n",
    "        - D'Agostino-Pearson's normality test<br>\n",
    "        ```scipy.stats.normaltest```\n",
    "        - Shapiro-Wilik Test<br>\n",
    "        ```scipy.stats.shapiro```\n",
    "    2. **Test for Homogeneity of Variance**\n",
    "        - Levene's Test<br>\n",
    "         ```scipy.stats.levene```\n",
    "\n",
    "    3. **Choose appropriate test based upon 1. and 2.** <br> \n",
    "    \n",
    "    \n",
    "* ***Aim 3: To perform post-hoc painrwise comparison testing to determine which level of discounts affect quantity and if any discount has a greater effect than the others.***\n",
    "\n",
    "     - Tukey's test for multiple pairwise comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9GqTizvBRo8"
   },
   "source": [
    "_________\n",
    "## ***H2, Aim 1 : To select the proper dataset for analysis  and generate data groups for testing.***\n",
    "\n",
    "#### Importing Method\n",
    "- Used sqlalchemy to create engine to connect to Northwind_small.sqlite.\n",
    "- used ```pd.read_sql_query('SELECT * FROM OrderDetail',engine)``` to directly read db into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = '/content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite'\n",
    "# filepath = 'Northwind_small.sqlite'\n",
    "#-------------------\n",
    "# Testing minimal version of prior code\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "# from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "\n",
    "engine = create_engine('sqlite:///'+filepath,echo=True);\n",
    "inspector = inspect(engine);\n",
    "db_tables = inspector.get_table_names();\n",
    "print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to test hypothesis one, so we need OrderDetail table.\n",
    "table_to_test = \"OrderDetail\"\n",
    "df = pd.read_sql_query(\"SELECT * FROM OrderDetail\",  engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSqmOZAHTcNQ"
   },
   "source": [
    "### Aim 1.1: Calculating order totals, adding discount groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply calc_product_price to every row \n",
    "df_price = df.apply(lambda x: calc_product_price(x),axis=1)\n",
    "# df_price['order_total'] = None\n",
    "\n",
    "# Apply_calc_order_total to every row\n",
    "df_price = df_price.apply(lambda x: calc_order_total(x,df_price), axis=1)  \n",
    "df_price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzQo_3AAxHGj"
   },
   "source": [
    "### Aim 1.2: EDA on dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define groups\n",
    "# A: discounted df_price.groupby('discounted_order').get_group(1)\n",
    "# B: full price df_price.groupbt('discounted_order').get_group(0)\n",
    "grpA = df_price.groupby('discounted_order').get_group(1)\n",
    "grpB = df_price.groupby('discounted_order').get_group(0)\n",
    "\n",
    "kwargs = {'title_var':'Order Totals'}\n",
    "fig, ax =plot_wide_kde_thin_bar(grpB['order_total'],'Full Price',grpA['order_total'],'Discounted',**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure of save_for_user is True\n",
    "if save_for_user == True:\n",
    "    fig.savefig(fig_filepath+'H2_kde_bar_black.png',dpi=300,facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Separating groups from dfH (dataframeHypothesis)\n",
    "df_temp = df_price[['OrderId','order_total','discounted_order']]\n",
    "\n",
    "unique_orders = df_temp['OrderId'].unique()\n",
    "\n",
    "len(unique_orders)\n",
    "keep_me=[['OrderId','order_total','discounted_order']]\n",
    "\n",
    "for u in unique_orders:\n",
    "#     grouped = dfH.groupby('OrderId').get_group(u)\n",
    "#     keep_me.append(grouped.iloc[0,:])\n",
    "    grouped_idx = df_temp.groupby('OrderId').get_group(u).index\n",
    "    keep_me.append(df_temp.loc[grouped_idx[0]])\n",
    "dfH = list2df(keep_me)\n",
    "dfH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfH[['order_total','discounted_order']].groupby('discounted_order').describe()#.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding group definitions\n",
    "grpA = \"['discounted_order'] == True\"\n",
    "grpB = \"['discounted_order'] == False\"\n",
    "\n",
    "# Using eval to create df_A and df_B; will allow us to fill in our results \n",
    "# with the exact definitions of Group A and Group B\n",
    "df_A = eval(f'dfH.loc[dfH{grpA}]')\n",
    "df_B = eval(f'dfH.loc[dfH{grpB}]')\n",
    "\n",
    "# dfj_D = dfH.loc[dfj['Discount']>0] \n",
    "# dfj_F = dfH.loc[dfj['Discount']==0]\n",
    "df_A.describe(),'--'*25,df_B.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZfc0jqTxeNA"
   },
   "source": [
    "## ***H2, Aim 2: Select the appropriate t-test based on tests for the assumptions of normality and homogeneity of variance.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding table to collect summary statistics\n",
    "H2_tests = [['Group','TestName','Test Purpose','stat','p','p<.05?']  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaXaorm5BRpM"
   },
   "source": [
    "### Aim 2.1: Test for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Testing normality\n",
    "test_purpose = 'Normality'\n",
    "test_to_run = 'normaltest'\n",
    "\n",
    "arrA = np.array(df_A['order_total'])\n",
    "arrB = np.array(df_B['order_total'])\n",
    "\n",
    "statA, pA = eval(test_to_run)(arrA)\n",
    "statB, pB = eval(test_to_run)(arrB)\n",
    "\n",
    "print(f'A:stat={statA}, p={pA}')\n",
    "print(f'B:stat={statB}, p={pB}')\n",
    "\n",
    "H2_tests.append(['A: '+grpA,test_to_run,test_purpose,statA,pA,pA<.05])\n",
    "H2_tests.append(['B: '+grpB,test_to_run,test_purpose,statB,pB,pB<.05])\n",
    "# list2df(H1_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H2_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-RHTuiWBRpR"
   },
   "source": [
    "### Aim 2.2 Test for Homogeneity of Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test for homo var\n",
    "test_to_run = 'stats.levene'\n",
    "stat,p = eval(test_to_run)(arrA,arrB,center='median')\n",
    "\n",
    "H2_tests.append(['A&B',test_to_run,'Equal Var',stat,p,p<.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H2_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OCGqU7PyiIc"
   },
   "source": [
    "### Aim 2.3 Selecting the appropriate test to perform.\n",
    "- We failed both normaltests\n",
    "- We passed Levene's test for homo. of variance\n",
    "- Need non-parametric 2sample ttest\n",
    "    - [**Mann-Whitney U test**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxxTNQymBRpX"
   },
   "source": [
    "#### Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann Whitney U test+\n",
    "import scipy.stats as stats\n",
    "# arrA was discounted, arrB was non-discounted\n",
    "test_to_run = 'stats.mannwhitneyu'\n",
    "stat, p = eval(test_to_run)(arrA,arrB,alternative='greater')\n",
    "stat, p\n",
    "H2_tests.append(['A vs. B',test_to_run, 'H2_sig', stat,p,p<.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H2_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9BLLITiBRpb"
   },
   "source": [
    "#### Calculating Effect Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Cohens d\n",
    "d = Cohen_d(arrA,arrB)\n",
    "print(f\"Cohen's d={round(d,3)}\")\n",
    "H2_tests.append(['A vs. B',\"Cohen's d\",'Effect Size',d,'--','--'])\n",
    "table_H2_test_results = list2df(H2_tests)\n",
    "\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H2_tests),'table_H2_test_results')\n",
    "#     table_H2_test_results.to_csv(data_filepath+'table_H2_test_results.csv')\n",
    "#     table_H2_test_results.to_excel(data_filepath+'table_H2_test_results.xlsx')\n",
    "# \n",
    "#     table_H2_test_results.round(4).to_html(data_filepath+'table_H2_test_results.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table of basic info on central tendency\n",
    "from scipy.stats import sem\n",
    "H2_mean_sem = [['H#','Group','mean','sem','std','n']]\n",
    "\n",
    "dataArr=arrA\n",
    "H2_mean_sem.append(['H2',f'A: {grpA}',np.mean(dataArr),sem(dataArr), np.std(dataArr),len(dataArr)])\n",
    "\n",
    "dataArr=arrB\n",
    "H2_mean_sem.append(['H2',f'B: {grpB}',np.mean(dataArr),sem(dataArr), np.std(dataArr),len(dataArr)])\n",
    "\n",
    "table_H2_mean_sem = list2df(H2_mean_sem)\n",
    "\n",
    "# Export csv if user sets save_for_user to True\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H2_mean_sem),'table_H2_mean_sem')\n",
    "#     table_H2_mean_sem.to_csv(data_filepath+'table_H2_mean_sem.csv') \n",
    "#     table_H2_mean_sem.to_excel(data_filepath+'table_H2_mean_sem.xlsx') \n",
    "\n",
    "#     table_H2_mean_sem.round(2).to_html(data_filepath+'table_H2_mean_sem.html')    \n",
    "\n",
    "# table_H2_mean_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table_H2_test_results)\n",
    "display(table_H2_mean_sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrLcQDMPyiIl"
   },
   "source": [
    "### Conclusions for Hypothesis 2:\n",
    "- We reject the null hypothesis that there is no effect of an order containing discounted items on the order total.\n",
    "- Therefore, we have found evidence that customers spend more money when they are buying at least 1 discounted item.\n",
    "\n",
    "- However, Cohen's d indicates it a small effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the re-sampled data\n",
    "\n",
    "# # IF want to take smaller sample from population:\n",
    "\n",
    "# pop_samp_disc = np.random.choice(pop_samp_disc,1000)\n",
    "# pop_samp_full = np.random.choice(pop_samp_full, 1000)\n",
    "with plt.style.context(('tableau-colorblind10')):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.title('Order Total  of Orders with Discounted Items vs Full Price')\n",
    "    \n",
    "    plt.hist(arrA,alpha = 0.5, bins=30,label='Discounted')\n",
    "    plt.hist(arrB,color='black', alpha = 0.5, bins=30,label='Full Price')\n",
    "\n",
    "    # Adding annotations\n",
    "    meanD = round(np.mean(arrA),3)\n",
    "    meanS = round(np.mean(arrB),3)\n",
    "    \n",
    "    plt.axvline(meanD, linestyle='--',label='Discounted Mean')\n",
    "    plt.text(meanD-1000,165,f'Mean:{meanD}',rotation=90)\n",
    "    \n",
    "    plt.axvline(meanS,color='k',linestyle='--',label='Full Price Mean')\n",
    "    plt.text(meanS+500,165,f'Mean:{meanS}',rotation=90)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run normality testing\n",
    "# stat,p = normaltest(pop_samp_full)\n",
    "# print(f'Normality: stat ={stat}, p = {p}')\n",
    "\n",
    "# Run ttest\n",
    "# tstat,tp = stats.ttest_ind(arrA, arrB)\n",
    "# print(f'T-test: stat ={tstat}, p = {tp}')#tstat,tp\n",
    "# ax.text("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBDZ_d_71Vat"
   },
   "source": [
    "_________\n",
    "\n",
    "# **Hypothesis 3**\n",
    "> **Does the time of year affect quantity of items sold?**\n",
    "\n",
    "- $H_1$ = The month an order is placed relates to either a higher or lower mean quantity of items sold.\n",
    "- $H_0$ = The month of an order has no affect on the mean quantity of items sold.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PaYTrUi1YH0"
   },
   "source": [
    "## ***H3, Aim 1.1 : To select the proper dataset for analyiss  and generate data groups for testing.***\n",
    "\n",
    "#### Importing Method\n",
    "- Use sqlalchemy to create engine to connect to Northwind_small.sqlite.<br>\n",
    "```python\n",
    "df_ord = pd.read_sql_query(\"SELECT * FROM OrderDetail JOIN [Order]  ON [Order].Id = OrderDetail.OrderId\", engine)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session, sessionmaker\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/', force_remount=True)\n",
    "\n",
    "# filepath = '/content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite'\n",
    "filepath = 'Northwind_small.sqlite'\n",
    "engine = create_engine('sqlite:///'+filepath,echo=True)\n",
    "inspector = inspect(engine);\n",
    "\n",
    "# df_employee = pd.read_sql_query(\"SELECT Id, Title, LastName, HireDate , BirthDate  FROM [EMPLOYEE]\", engine )\n",
    "# df_cust_ord = pd.read_sql_query(\"SELECT *FROM [Order] JOIN [Customer] ON [Customer].Id = [Order].CustomerId\", engine)\n",
    "print(inspector.get_table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8M9eXYeYwMo"
   },
   "source": [
    "### Extract SQL tables and clean/combine \n",
    "- Original extract used for analysis.\n",
    "- Secondary method was added to export a larger csv for external plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractiong of specific, required data from database. \n",
    "df_ord = pd.read_sql_query(\"SELECT * FROM OrderDetail JOIN [Order]  ON [Order].Id = OrderDetail.OrderId\", engine)\n",
    "# df_ord = pd.concat([df_ord, df_employee['Title']], axis=1)\n",
    "df_ord.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unwanted columns\n",
    "df_ord.drop(['OrderId', 'ShipName',\n",
    "             'ShippedDate', 'RequiredDate',\n",
    "             'ShipVia','ShipAddress',\n",
    "             'RequiredDate','ShippedDate',\n",
    "             'ShippedDate', 'ShipCity',\n",
    "             'ShipCountry','ShipRegion',\n",
    "             'Freight'],inplace=True, axis=1)\n",
    "\n",
    "# relabel the Id column so they have unique names\n",
    "df_ord.columns = ['Id', 'ProductId', 'UnitPrice',\n",
    "                  'Quantity', 'Discount', 'OrderId',\n",
    "                  'CustomerId', 'EmployeeId',\n",
    "                  'OrderDate', 'ShipPostalCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating new features based on unit pricing (df_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply calc_product_price to every row \n",
    "df_price = df_ord.apply(\n",
    "    lambda x: calc_product_price(\n",
    "        x),axis=1)\n",
    "\n",
    "# Apply_calc_order_total to every row\n",
    "df_price = df_price.apply(\n",
    "    lambda x: calc_order_total(\n",
    "        x,df_price), axis=1)  \n",
    "\n",
    "# Display output\n",
    "df_price.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Datetime columns for day of week and month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates to datetime\n",
    "df_price['OrderDate'] = pd.to_datetime(\n",
    "                    df_price.OrderDate )\n",
    "\n",
    "(df_price.OrderDate.sort_values().min(),\n",
    " df_price.OrderDate.sort_values().max())# seeing date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for day of week and month\n",
    "df_price['OrderDate'] = pd.to_datetime(\n",
    "                    df_price.OrderDate) \n",
    "\n",
    "df_price['week_day'] = df_price['OrderDate'].dt.dayofweek\n",
    "df_price['month'] = df_price['OrderDate'].dt.month\n",
    "\n",
    "# df_price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining criteria for dividing the calendar yaer\n",
    "criteria = df_price['month'] <= 6 # boolean selector used to select parts of df for possible plotting options\n",
    "\n",
    "#drop duplicate orders based on OrderId to not inflate or deflate order_total mean\n",
    "# split in two for exploring subsets\n",
    "df_month6 = df_price.loc[criteria,\n",
    "                         ['month','order_total',\n",
    "                          'Discount','OrderId']\n",
    "                        ].drop_duplicates(['OrderId'])\n",
    "\n",
    "df_month12 = df_price.loc[~criteria,\n",
    "                          ['month','order_total',\n",
    "                           'Discount','OrderId']\n",
    "                         ].drop_duplicates(['OrderId'])\n",
    "\n",
    "#Did use this variable\n",
    "total_order = pd.concat(\n",
    "                [df_month6['order_total'],\n",
    "                df_month12['order_total']],axis=0)\n",
    "\n",
    "# verifying correct lengths\n",
    "len(total_order) == (len(df_month6['order_total']) \n",
    "                   + len(df_month12['order_total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting out final working dataframe for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = df_price.loc[:,\n",
    "                       ['month', 'order_total',\n",
    "                        'Discount', 'OrderId']\n",
    "                       ].drop_duplicates(['OrderId'])\n",
    "df_year.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary containing keys and values representing months to assist in plotting and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of month names (strings) for labels\n",
    "months = ['jan','feb', 'mar', 'apr', 'may' , 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']#creating label names\n",
    "months = [x.title() for x in months]\n",
    "month_dict = dict(zip( list(range(1,len(months)+1)),months)) # zip the two into a dictionary\n",
    "\n",
    "# months = ['jan','feb',\n",
    "#           'mar','apr',\n",
    "#           'may' ,'jun',\n",
    "#           'jul', 'aug',\n",
    "#           'sep', 'oct',\n",
    "#           'nov', 'dec'\n",
    "#          ]\n",
    "# #make corresponding list of integers from 1 = 'jan' through 12 = 'dec'\n",
    "# month_code = list(range(1,len(months)+1))\n",
    "# month_dict = dict(zip(month_code,months))\n",
    "# zip the two into a dictionary\n",
    "month_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month name\n",
    "for k,v in month_dict.items():\n",
    "      df_year.loc[(df_year['month'] == k), 'month_name'] = v \n",
    "        \n",
    "df_yearM = df_year.copy()\n",
    "df_yearM.head(3), df_yearM.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def_drop_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_drop_regex(DF, regex_list):\n",
    "    '''Use a list of regex to remove columns names. Returns new df.\n",
    "    \n",
    "    Parameters:\n",
    "        DF -- input dataframe to remove columns from.\n",
    "        regex_list -- list of string patterns or regexp to remove.\n",
    "    \n",
    "    Returns:\n",
    "        df_cut -- input df without the dropped columns. \n",
    "        '''\n",
    "    df_cut = DF.copy()\n",
    "    \n",
    "    for r in regex_list:\n",
    "        \n",
    "        df_cut = df_cut[df_cut.columns.drop(list(df_cut.filter(regex=r)))]\n",
    "        print(f'Removed {r}\\n')\n",
    "        \n",
    "    return df_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary extraction method for larger dataset to export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting additional information to export for external plotting. \n",
    "DB_Order = pd.read_sql_table('Order',engine);\n",
    "DB_OrderDetail = pd.read_sql_table('OrderDetail',engine);\n",
    "print(f\"DB_Order columns:{DB_Order.columns}n\\n DB_OrderDetail columns: {DB_OrderDetail.columns}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE  DB_Orderm rename index, remove unwanted columns\n",
    "DB_Order.rename({'Id':'OrderId','OrderDate':'OrderPlaced'},axis=1,inplace=True)\n",
    "\n",
    "# Drop unwanted columns from DB_Orde to make df_Order\n",
    "regex_to_drop = ['Date','Freight']\n",
    "\n",
    "df_Order = df_drop_regex(DB_Order, regex_to_drop)\n",
    "df_Order.rename({'OrderPlaced':'OrderDate'},axis=1,inplace=True)\n",
    "# df_Order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING IMPORT DB_OrderDetail and cleaned df_Order\n",
    "df_merged = DB_OrderDetail.merge(df_Order, on='OrderId',copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming dataframes for consistency\n",
    "df_order_geo = df_merged.copy()\n",
    "# df_order_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove some columns from df_order_geo\n",
    "\n",
    "# df_order_geo.drop(['OrderId', 'ShipName', 'ShippedDate', 'RequiredDate', 'ShipVia','ShipAddress',\n",
    "#              'RequiredDate','ShippedDate', 'Freight'],inplace=True, axis=1)\n",
    "# # relabel the Id column so they have unique names\n",
    "# df_order_geo.rename(columns={'Id':'OrderId_'},inplace=True )\n",
    "# df_order_geo.info()\n",
    "# # print(f'df_ord.info():\\n {df_ord.info()}\\n\\ndf_order_geo.info():\\n{df_order_geo.info()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply calc_product_price to every row \n",
    "df_price_geo = df_order_geo.apply(lambda x: calc_product_price(x),axis=1)\n",
    "\n",
    "# Apply_calc_order_total to every row\n",
    "df_price_geo = df_price_geo.apply(lambda x: calc_order_total(x,df_price_geo), axis=1)  \n",
    "# df_price_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates to datetime\n",
    "df_price_geo['OrderDate'] = pd.to_datetime(df_price_geo.OrderDate )\n",
    "df_price_geo.OrderDate.agg(['mean','max','min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for day of week and month\n",
    "df_price_geo['OrderDate'] = pd.to_datetime(df_price_geo.OrderDate)\n",
    "\n",
    "df_price_geo['week_day'] = df_price_geo['OrderDate'].dt.dayofweek\n",
    "df_price_geo['month'] = df_price_geo['OrderDate'].dt.month\n",
    "\n",
    "# df_price_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of month names to map onto dataframe.\n",
    "months = ['jan','feb', 'mar', 'apr', 'may' , 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']#creating label names\n",
    "months = [x.title() for x in months]\n",
    "month_dict = dict(zip( list(range(1,len(months)+1)),months)) # zip the two into a dictionary\n",
    "\n",
    "# MAP THE MONTH_DICT ONTO NEW COLUMN month_name\n",
    "df_price_geo['month_name'] = df_price_geo['month'].map(month_dict)\n",
    "df_price_geo['month_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save df_price_Geo \n",
    "# # Save figure of save_for_user is True\n",
    "# if save_for_user == True:\n",
    "# # save = input(prompt='Would you like to export the dataframe above? (y/n)\\n')\n",
    "# # if save.lower()=='y':\n",
    "#     filename ='df_H3_price_w_dates_products.csv'\n",
    "#     df_H3_price_w_dates_products = df_price_geo.copy()\n",
    "#     df_H3_price_w_dates_products.to_csv(data_filepath+filename)\n",
    "#     df_H3_price_w_dates_products.to_excel(data_filepath+'df_H3_price_w_dates_products.xlsx')\n",
    "\n",
    "#     print(f'df_price exported and saved as {filename}...')\n",
    "#     print(f'if you are running this on Colab:...\\nOpen File sidebar, click Refresh, right click on {filename} to Download.')\n",
    "    \n",
    "#     filename ='df_H3_price_w_dates_orders.csv'\n",
    "#     df_H3_price_w_dates_orders = df_H3_price_w_dates_products.drop_duplicates(subset=['OrderId'])\n",
    "#     df_H3_price_w_dates_orders.to_csv(data_filepath+filename)\n",
    "#     df_H3_price_w_dates_orders.to_excel(data_filepath+'df_H3_price_w_dates_orders.xlsx')\n",
    "\n",
    "#     print(f'if you are running this on Colab:...\\nOpen File sidebar, click Refresh, right click on {filename} to Download.')\n",
    "\n",
    "# else:\n",
    "#     print('No .csv exported.')\n",
    "    \n",
    "    \n",
    "# # print(f'The cell above is currently commented out, uncomment to export csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_price_geo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOHvLslMTz2u"
   },
   "source": [
    "### EDA\n",
    " - Extraction workflows merge here. \n",
    " - define uselful functions for visualizations\n",
    " - Plotting and visualizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking which iteration of df_price to use \n",
    "df_year = df_price_geo.copy()\n",
    "#df_year = df_priceM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Adding df_year_orders = df_price_geo should ensure that the rest of mike's code runs\n",
    "df_year_orders = df_price_geo.drop_duplicates(subset=['OrderId']).copy()\n",
    "df_year_orders.set_index('OrderId',inplace=True,verify_integrity=True)\n",
    "# df_year_orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded and misleading formerly product-associated columns\n",
    "vars_to_drop = ['EmployeeId','ShipVia','ShipAddress','ShipRegion','ProductId','ShipName','Id','price','OnSale']\n",
    "df_year_orders.drop(vars_to_drop,axis=1,inplace=True)\n",
    "# df_year_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-IJsfaeZShd"
   },
   "source": [
    "### plot initial order totals by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables to be plotted\n",
    "x = df_year_orders['month']\n",
    "y = df_year_orders['order_total']\n",
    "ticks = [v for v in month_dict.values()] \n",
    "title = 'Order totals per month with or without discounts'\n",
    "hue = df_year_orders['Discount'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = make_violinplot(x,y,title,hue, ticks)\n",
    "if save_for_user == True:\n",
    "    plt.savefig(fig_filepath+'violin - order totals per month by discount.png', dpi=300)\n",
    "# fig = plt.gcf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stripplot(x,y,title,hue,ticks)\n",
    "if save_for_user == True:\n",
    "    plt.savefig(fig_filepath+'stripplot - order totals per month.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4ODtjI71c03"
   },
   "source": [
    "## ***H3, Aim 2 : Test for assumptions of normality and equal variance, then choose test***\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkFmrtPc1a8I"
   },
   "source": [
    "\n",
    "* ***Aim 2: Select the appropriate t-test based on tests for the assumptions of normality and homogeneity of variance.***\n",
    "    1. **Test for Normality**<br>\n",
    "        - [Normaltest/ D’Agostino and Pearson’s](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html)\n",
    "<br>\n",
    "        ```scipy.stats.normaltest```\n",
    "        - Shapiro-Wilik Test<br>\n",
    "        ```scipy.stats.shapiro```\n",
    "    2. **Test for Homogeneity of Variance**<br>\n",
    "        - [Levene's Test](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.levene.html)<br>         ```scipy.stats.levene```\n",
    "    3. **Choose appropriate test based upon 1. and 2.** \n",
    "        - [Mann Whitney U Test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html)<br>  - non parametric equivalent of ANOVA)<br>\n",
    "    ```stats.mannwhitneyu```<br>\n",
    "        - Normal: 2 sample t-test\n",
    "        - Welch's t-test (Jeff)\n",
    "        \n",
    "    4.  Calculate effect size, post-hoc tukeys tests.\n",
    "        - Effect size: [cohen's d](https://stackoverflow.com/questions/21532471/how-to-calculate-cohens-d-in-python)\n",
    "          \n",
    "* ***Aim 3: To perform post-hoc pairwise comparisons for level of discount***      \n",
    "    5. If significant result, follow up with post-hoc tests\n",
    "        - [Tukey's](https://www.statsmodels.org/stable/generated/statsmodels.stats.multicomp.pairwise_tukeyhsd.html)\n",
    "        ```statsmodels.stats.multicomp.pairwise_tukeyhsd```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFMslgjQqhPW"
   },
   "source": [
    "#### Histograms month vs all months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_histograms(df_year, 'month', month_dict, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_histograms_from_sample(df_year_orders, 'month', month_dict, 4, 3)\n",
    "# df_year_orders[['Quantity','Discount','price','order_total']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_year_orders.groupby('month_name').agg(['mean','std','min','max','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwvBZywE0V0B"
   },
   "source": [
    "### Aim 2.1: Test for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to test stats\n",
    "dict_to_test ={}\n",
    "for month in list(df_year_orders['month_name'].unique()):\n",
    "    dict_to_test[month] = df_year_orders.groupby('month_name').get_group(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in dict_to_test.items():\n",
    "    print(f'{k} has: {len(v)} rows.')\n",
    "# len(dict_to_test.values())\n",
    "# dict_to_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create H3_tests list to receive all test results for this hypothesis:\n",
    "H3_tests = [['Group','TestName','Test Purpose','stat','p','p<.05?']  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use normaltest for D'Agostino Pearson's test\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# Recursively go through months and then test each monthths\n",
    "for month,  df  in dict_to_test.items(): #month = key, df = values\n",
    "\n",
    "    arrA = dict_to_test[month]['order_total']\n",
    "\n",
    "    #1. Test for normality\n",
    "    test_purpose = 'Normality'\n",
    "    test_to_run = 'normaltest'\n",
    "\n",
    "    arrA = np.array(arrA)\n",
    "    statA, pA = eval(test_to_run)(arrA)\n",
    "\n",
    "    # Append the result\n",
    "    H3_tests.append([month, test_to_run, test_purpose ,statA, pA, pA<0.05])\n",
    "\n",
    "# Adding a test for the year as a whole. \n",
    "arrB = np.array(df_year_orders['order_total'])\n",
    "stat, p = eval(test_to_run)(arrB)\n",
    "\n",
    "# Append the results.\n",
    "H3_tests.append(['Total Pop', test_to_run, test_purpose,stat, p,p<0.05])\n",
    "\n",
    "# Display Results\n",
    "H3_results_norm = list2df(H3_tests)\n",
    "H3_results_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNlFwil16GUb"
   },
   "source": [
    "### Aim 2.2: Test for Homogneity of Variance\n",
    "- Levenes Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "for month,  df  in dict_to_test.items(): #month = key, df = values\n",
    "    \n",
    "    # Create array A for month and array B for all other months.\n",
    "    arrA = dict_to_test[month]['order_total']\n",
    "    arrB = df_year_orders[df_year_orders['month_name']!= month]['order_total']\n",
    "  \n",
    "    arrA = np.array(arrA)\n",
    "    arrB = np.array(arrB)\n",
    "    \n",
    "    \n",
    "    #1. Test for equal variance. \n",
    "    test_to_run = 'levene'\n",
    "    test_purpose = 'Equal Variance'\n",
    "\n",
    "    # Run Levene's test\n",
    "    stat, p = eval(test_to_run)(arrA,arrB,center='median')\n",
    "    \n",
    "    # Append Results\n",
    "    H3_tests.append([f'{month} vs. Other Months', test_to_run, test_purpose ,stat, p, p<0.05])\n",
    "\n",
    "list2df(H3_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqTFEWNju4DM"
   },
   "source": [
    "### Aim 2.3. Choose Appropriate Test Based on Testing Assumptions\n",
    "\n",
    "- Note: We did not meet the criteria for normality or equal variance. \n",
    "- We also have more than 2  groups, so cannot do the Mann-Whitney U test. \n",
    "- Decided to run both a M.W.U test and Tukey's\n",
    "    - MWU to analyze each month vs the other months.\n",
    "    - Tukey's for pairwise comparisons between months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mann-Whitney U test of months vs year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "for month, df  in dict_to_test.items(): #month = key, df = values\n",
    "\n",
    "    arrA = dict_to_test[month]['order_total']\n",
    "    arrB = df_year.loc[df_year['month_name']!= month,'order_total']\n",
    "\n",
    "    test_to_run = 'mannwhitneyu'\n",
    "    test_purpose = 'Hi sig.'\n",
    "\n",
    "    arrA = np.array(arrA).reshape(-1,1)\n",
    "    arrB = np.array(arrB).reshape(-1,1)\n",
    "\n",
    "    stat, p = eval(test_to_run)(arrA,arrB,alternative='two-sided')\n",
    "    \n",
    "    print(f'{month}:',stat, p)\n",
    "    \n",
    "    H3_tests.append([f'{month} vs. Other Months', test_to_run, test_purpose ,stat, p, p < .05])\n",
    "\n",
    "list2df(H3_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating effect size wiht Cohen's d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating effect sizes\n",
    "d_dict = {}\n",
    "for month,  df  in dict_to_test.items(): #month = key, df = values\n",
    "\n",
    "    arrA = dict_to_test[month][['order_total']]\n",
    "    arrB = df_year[df_year['month_name']!= month]['order_total']\n",
    "\n",
    "    test_to_run = 'Cohen_d'\n",
    "    test_purpose = 'efect Size'\n",
    "\n",
    "    arrA = np.array(arrA)\n",
    "    arrB = np.array(arrB)\n",
    "\n",
    "    d = Cohen_d(arrA, arrB)\n",
    "    \n",
    "    d_dict[month] = d\n",
    "    \n",
    "    print(f'{month}:',d)\n",
    "    H3_tests.append([f'{month}', test_to_run, test_purpose ,stat])\n",
    "list2df(H3_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export H3 test resutls.\n",
    "table_H3_test_results = list2df(H3_tests)\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H3_tests),'table_H3_test_results')\n",
    "#     table_H3_test_results.to_csv(data_filepath+'table_H3_test_results.csv')\n",
    "#     table_H3_test_results.to_excel(data_filepath+'table_H3_test_results.xlsx')\n",
    "#     table_H3_test_results.round(4).to_html(data_filepath+'table_H3_test_results.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azg_e4Sjylcg"
   },
   "source": [
    "### Use Tukey's Pairwise Multiple Comparison test.\n",
    "```statsmodels.stats.multicomp.pairwise_tukeyhsd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tukey's test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd as tukey\n",
    "\n",
    "# Defome the dataframe containing column of interest and group labels. \n",
    "df_test_hypothesis = df_year_orders[['order_total','month_name','week_day']]\n",
    "grp_labels = df_test_hypothesis['month_name']\n",
    "\n",
    "# Run tukey's test\n",
    "tukey_results = tukey(df_test_hypothesis['order_total'], grp_labels, 0.05)\n",
    "\n",
    "\n",
    "# Save the results into a dataframe\n",
    "table_H3_tukey = tukey_table(tukey_results)\n",
    "\n",
    "# save for user\n",
    "if save_for_user==True:\n",
    "    export_data(table_H3_tukey,'table_H3_tukey_full')\n",
    "    export_data(table_H3_tukey.loc[table_H3_tukey['Reject']==True] ,'table_H3_tukey_sig_only')\n",
    "\n",
    "# Display any significant pairwise comparisons. \n",
    "tukey_results.summary()\n",
    "# display(table_H3_tukey.loc[table_H3_tukey['reject']==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There were no significant pairwise comparisons.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6J9p4XL3Lnr"
   },
   "source": [
    "#### SAVE THIS: using groupby to get mean, sem, for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANT TO CALCULATE MEAN AND SEM FOR BAR PLOT FOR DF\n",
    "# Calc Standard Error of the Mean for PLotting.\n",
    "from scipy.stats import sem\n",
    "\n",
    "d_plot={}\n",
    "d_plot['mean'] = df_year_orders.groupby(['month'])['order_total'].mean()\n",
    "d_plot['sem'] = df_year_orders.groupby(['month'])['order_total'].sem()\n",
    "d_plot['std'] = df_year_orders.groupby(['month'])['order_total'].std()\n",
    "d_plot['n'] = df_year_orders.groupby(['month'])['order_total'].count()\n",
    "\n",
    "df_plot = pd.DataFrame.from_dict(d_plot)\n",
    "\n",
    "# Convert month index to month_num column\n",
    "df_plot['month_num'] = df_plot.index\n",
    "\n",
    "# Use month_dict to get months labeled with names\n",
    "df_plot['month_name']=df_plot['month_num'].map(month_dict)\n",
    "df_plot.set_index('month_name',drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export table with mean and sem for all months\n",
    "H3_mean_sem = [['H#','Group','mean','sem','std','n']]\n",
    "\n",
    "for month in df_plot.index:\n",
    "   \n",
    "    h = 'H3'\n",
    "    group = month.title()\n",
    "    mo_mean = df_plot.loc[month,'mean']\n",
    "    mo_sem = df_plot.loc[month,'sem']\n",
    "    mo_std = df_plot.loc[month,'std']\n",
    "    n = df_plot.loc[month,'n']\n",
    "    H3_mean_sem.append([h, group, mo_mean, mo_sem, mo_std, n])\n",
    "\n",
    "\n",
    "# Export csv if user sets save_for_user to True\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H3_mean_sem),'table_H3_mean_sem')\n",
    "\n",
    "#     table_H3_mean_sem.to_csv(data_filepath+'table_H3_mean_sem.csv')    \n",
    "#     table_H3_mean_sem.to_excel(data_filepath+'table_H3_mean_sem.xlsx')        \n",
    "#     table_H3_mean_sem.round(2).to_html(data_filepath+'table_H3_mean_sem.html')    \n",
    "\n",
    "table_H3_mean_sem = list2df(H3_mean_sem)   \n",
    "display(table_H3_mean_sem.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(table_H3_test_results)\n",
    "# display(table_H3_tukey)\n",
    "display(table_H3_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final results for H3_tests\n",
    "# H3_tests_table = list2df(H3_tests)\n",
    "table_H3_test_results.groupby('TestName').get_group('mannwhitneyu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUqcqJPZylcn"
   },
   "source": [
    "## ***Conclusions for Hypothesis 3:***\n",
    "1. **We first used D'Agostino & Perason's test (normaltest) to check for normal distributions**\n",
    "    -  Both groups had p-values very far below our $\\alpha=0.05$  (Rows 0,1 below)\n",
    "        - We reected the null hypothesis that the groups came from a population with a normal distribution. \n",
    "2. **We tested for equal variances using Levene's test. **\n",
    "    - Levene's test had a signifcant p-value, so we rejected the hypothesis that the groups have equal variances\n",
    "    \n",
    "3. **We then concluded we need a non-parametric test, so we first used the Mann-Whitney U test to compare each month vs the other 11 months**. \n",
    "    -  Several months were significantly different than the rest of the year:\n",
    "        - Jul, Aug, Sep, Feb, Mar, Apr\n",
    "    -  \n",
    "4. **We then did pairwise comparisons of all months using Tukey's test**\n",
    "    - We could not reject the null hypothesis that month affects the quantity sold. \n",
    "    - It is difficult to interpret the contradicting results of the Mann Whitney U and the Tukey's test.\n",
    "        - But since Tukey's test corrects for multiple comparisons, it should be trusted over M.W.U.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Figure H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ax.bar(x=df_plot.index, height= df_plot['mean'], yerr=df_plot['sem'])\n",
    "\n",
    "with plt.style.context('dark_background'):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    color = ['red','orange','yellow','green','blue','indigo','violet']\n",
    "    \n",
    "    err_kws = {'ecolor': 'w', 'capsize':5,\n",
    "               'elinewidth': 2 ,'capthick': 2}\n",
    "    \n",
    "    fontTitle = {'fontfamily':'serif',\n",
    "               'fontweight':'medium',\n",
    "               'fontsize':16}\n",
    "    \n",
    "    fontAxis = {'fontfamily':'serif',\n",
    "               'fontweight':'medium',\n",
    "               'fontsize':12}\n",
    "    labels = [str(i).title() for i in df_plot.index]\n",
    "    ax.bar(x = labels,width=0.6, height = df_plot['mean'], yerr = df_plot['sem'], error_kw=err_kws,\n",
    "          edgecolor='white', linewidth = 2,color = color)\n",
    "    \n",
    "    ax.set_title('Order Totals By Month',fontdict=fontTitle)\n",
    "    ax.set_ylabel('Average Order Total',fontdict=fontAxis)\n",
    "    ax.set_xlabel('Month', fontdict=fontAxis)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure of save_for_user is True\n",
    "if save_for_user == True:\n",
    "    fig.savefig(fig_filepath+'H3_Month_Bar_mean_sem_black.png',dpi=300,facecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSv7gEWPuXTn"
   },
   "source": [
    "___\n",
    "# **Hypothesis 4** \n",
    "\n",
    "> Do different countries respond to discounts more than others? \n",
    "\n",
    "$H_1$: Different countries purchase different quantities of discounted vs non discounted products. \n",
    "\n",
    "$H_0$: All countries purchase the same quantities of discounted vs non discounted products. \n",
    "\n",
    "**Specific Aims:**\n",
    "\n",
    "* ***Aim 1:To select the proper dataset for analysis, perform EDA, and generate data groups for testing.***\n",
    "    - Used sqlalchemy and pandas.read_sql_query()\n",
    "    query = \n",
    "\n",
    "* ***Aim 2: Select the appropriate t-test based on tests for the assumptions of normality and homogeneity of variance.***\n",
    "    1. **Test for Normality**\n",
    "        - D'Agostino-Pearson's normality test<br>\n",
    "        ```scipy.stats.normaltest```\n",
    "        - Shapiro-Wilik Test<br>\n",
    "        ```scipy.stats.shapiro```\n",
    "    2. **Test for Homogeneity of Variance**\n",
    "        - Levene's Test<br>\n",
    "         ```scipy.stats.levene```\n",
    "\n",
    "    3. **Choose appropriate test based upon 1. and 2.** \n",
    "\n",
    "\n",
    "* ***Aim 3: To perform post-hoc painrwise comparison testing to determine which level of discounts affect quantity and if any discount has a greater effect than the others.***\n",
    "     - Tukey's test for multiple pairwise comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnY9sSjWuKZL"
   },
   "source": [
    "## ***H4, Aim 1.1 : To select the proper dataset for analyiss  and generate data groups for testing.***\n",
    "\n",
    "#### Importing Method\n",
    "- Used sqlalchemy to create engine to connect to Northwind_small.sqlite.\n",
    "- Read tables directly into dataframes<br>\n",
    "```python\n",
    "DB_Order = pd.read_sql_table('Order',engine);\n",
    "DB_OrderDetail = pd.read_sql_table('OrderDetail',engine);```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Order and OrderDetail Tables\n",
    "DB_Order = pd.read_sql_table('Order',engine);\n",
    "DB_OrderDetail = pd.read_sql_table('OrderDetail',engine);\n",
    "print(f\"DB_Order columns:{DB_Order.columns}\",'n\\n'\n",
    "      \"DB_OrderDetail columns:\",'n\\n'\n",
    "     f\"{DB_OrderDetail.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Id in DB_Order, rename index,and remove unwanted columns\n",
    "DB_Order.rename({'Id':'OrderId','OrderDate':'OrderPlaced'},axis=1,inplace=True)\n",
    "\n",
    "# Drop unwanted columns from DB_Orde to make df_Order\n",
    "regex_to_drop = ['Date','Freight']\n",
    "\n",
    "df_Order = df_drop_regex(DB_Order, regex_to_drop)\n",
    "df_Order.rename({'OrderPlaced':'OrderDate'},axis=1,inplace=True)\n",
    "df_Order.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4, Aim 1.2.1 Calculating Required Features \n",
    "- Calculating the sub-total per items per purchase, order_totals\n",
    "- Creating datetime columns with week_day, month, month_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING THE SUB TOTALS AMD ORDER TOTALS\n",
    "df_merged = DB_OrderDetail.merge(df_Order, on='OrderId',copy=True)\n",
    "df_order_geo = df_merged.copy()\n",
    "\n",
    "#Apply calc_product_price to every row \n",
    "df_price_geo = df_order_geo.apply(lambda x: calc_product_price(x), axis=1)\n",
    "\n",
    "# Apply_calc_order_total to every row\n",
    "df_price_geo = df_price_geo.apply(lambda x: calc_order_total(x,df_price_geo), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to datetime and add week_day and month columns\n",
    "# Create columns for day of week and month\n",
    "df_price_geo['OrderDate'] = pd.to_datetime(df_price_geo.OrderDate)\n",
    "df_price_geo['week_day'] = df_price_geo['OrderDate'].dt.dayofweek\n",
    "df_price_geo['month'] = df_price_geo['OrderDate'].dt.month\n",
    "\n",
    "## Replace month number codes with 3-letter abbreviations\n",
    "# Make list of month names to map onto dataframe.\n",
    "months = ['jan','feb', 'mar', 'apr', 'may' , 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']#creating label names\n",
    "months = [x.title() for x in months]\n",
    "month_dict = dict(zip( list(range(1,len(months)+1)),months)) # zip the two into a dictionary\n",
    "\n",
    "# MAP THE MONTH_DICT ONTO NEW COLUMN month_name\n",
    "df_price_geo['month_name'] = df_price_geo['month'].map(month_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_geo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4, Aim 1.2.2: Engineering Additional Features by Country\n",
    "- difference between quantity bought with and without discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renamed df for ease of typing & it is a comprehensize starting point\n",
    "df = df_price_geo[['Id','OrderId','Quantity','Discount','OrderDate','ShipCity','ShipCountry','price','OnSale','discounted_order','order_total','month','month_name','week_day']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each country's sales data into a dictionary in order to create New features for global discount effects on sales\n",
    "D_country = {}\n",
    "\n",
    "countries = list(df['ShipCountry'].unique())\n",
    "\n",
    "for country in countries:\n",
    "    \n",
    "    #check for discounts    \n",
    "    check_if_discounted = len(df.groupby('ShipCountry').get_group(country)['OnSale'].unique())\n",
    "    \n",
    "    if check_if_discounted < 2:\n",
    "        print(f'{country} did not have both discounted and non-discounted items.')\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Organize prices based on sales and fullprice    \n",
    "        D_country[country]={}\n",
    "\n",
    "        D_country[country]['df_sale'] = df.loc[df['OnSale']==1].groupby('ShipCountry').get_group(country)\n",
    "        \n",
    "        D_country[country]['df_fullprice'] = df.loc[df['OnSale']==0].groupby('ShipCountry').get_group(country) \n",
    "        \n",
    "        #Get total sales\n",
    "        D_country[country]['price']={}\n",
    "        D_country[country]['price']['OnSale'] = D_country[country]['df_sale']['price']\n",
    "        D_country[country]['price']['FullPrice'] = D_country[country]['df_fullprice']['price']\n",
    "        \n",
    "        #Get Quantity sold at discount/nondiscount\n",
    "        D_country[country]['quantity']={}\n",
    "        D_country[country]['quantity']['OnSale'] = D_country[country]['df_sale']['Quantity']\n",
    "        D_country[country]['quantity']['FullPrice'] = D_country[country]['df_fullprice']['Quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Discount-Preference Features to Explore\n",
    "- ```Price_Disc-Full```: Sum of all purchases of discounted items minus the sum of all purchases of full price items.\n",
    "- ```Price_Disc/Total```: Ratio of money spent on discounted products / total money spent on orders. \n",
    "- ```Quant_Disc-Full```: Sum of all discounted products' quantities minus sum of all full price products' quantities. \n",
    "- ```Quant_Disc/Total```: Ratio of quantities of discounted products / total quantity of all products ordered.\n",
    "- ```AvgPrice_Disc```: Average price of discounted items purchased (sum(price) / sum(quantity) of discounted items.\n",
    "- ```AvgPrice_full```: Average price of full price items purchased (sum(price) / sum(quantity) of full price items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for country-preference for discounted items. \n",
    "dC = D_country\n",
    "results = [['country','Price_Disc-Full','Price_Disc/Total','Quant_Disc-Full',\n",
    "            'Quant_Disc/Total','AvgPrice_Disc','AvgPrice_Full','AvgPrice/item']]\n",
    "\n",
    "# For each country:\n",
    "for c in dC.keys():\n",
    "    \n",
    "    # fill in temp list with the names of the data that will be placed in their location. \n",
    "    temp = ['country','Price_Disc-Full','Price_Disc/Total','Quant_Disc-Full',\n",
    "            'Quant_Disc/Total','AvgPrice_Disc','AvgPrice_Full','AvgPrice/item']\n",
    "    \n",
    "    calc = dC[c]\n",
    "    i=0\n",
    "    temp[i] = c   # country name\n",
    "    \n",
    "    i+=1\n",
    "    temp[i] = (calc['price']['OnSale'].sum()         # price sub\n",
    "               - calc['price']['FullPrice'].sum())\n",
    "    i+=1\n",
    "    temp[i] = (calc['price']['OnSale'].sum()        # price/total \n",
    "               / (calc['price']['FullPrice'].sum() \n",
    "               + calc['price']['OnSale'].sum()))\n",
    "    i+=1\n",
    "    temp[i] = (calc['quantity']['OnSale'].sum()      # quantitty sub\n",
    "               - calc['quantity']['FullPrice'].sum())\n",
    "    i+=1\n",
    "    temp[i] = (calc['quantity']['OnSale'].sum()      # quantity total\n",
    "               / (calc['quantity']['FullPrice'].sum() \n",
    "               + calc['quantity']['OnSale'].sum()))\n",
    "    i+=1\n",
    "    temp[i] = (calc['price']['OnSale'].sum()       # average spent on sale \n",
    "               / calc['quantity']['OnSale'].sum() )\n",
    "    i+=1                                           \n",
    "    temp[i] = (calc['price']['FullPrice'].sum()      # average cost full price \n",
    "               / calc['quantity']['FullPrice'].sum()) \n",
    "    i+=1\n",
    "    temp[i] = (calc['price']['FullPrice'].sum()  # average spent on all items\n",
    "               + calc['price']['OnSale'].sum() \n",
    "               / calc['quantity']['FullPrice'].sum() \n",
    "               + calc['quantity']['OnSale'].sum())\n",
    "    results.append(temp)\n",
    "\n",
    "# Display the engineered features.\n",
    "df_countries = list2df(results)\n",
    "df_countries.set_index('country',inplace=True)\n",
    "df_countries.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered additional features for comparing countries\n",
    "- ```AvgOrderPrice```: Mean of all order totals for a country.\n",
    "- ```PctOf-orders```: The % of all global orders placed by a country. \n",
    "- ```PctOf-Income ```: The % of all global income (order_totals) placed by a country. \n",
    "- ```orders2-Income```: _Comparing percentage of all orders / percentage on overall income_\n",
    "- ```DiscountGap```:  Country's AvgPrice_Full - AvgPrice_Discounted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.DataFrame(df.drop_duplicates(['OrderId']))\n",
    "\n",
    "# Making average price of sales for each country\n",
    "df_countries['AvgOrderPrice'] = (cleaned.groupby('ShipCountry')['order_total'].mean())\n",
    "\n",
    "\n",
    "#The % of all global orders placed by a country. \n",
    "df_countries['PctOf-orders'] = round((cleaned.groupby('ShipCountry')['OrderId'].count() \n",
    "                                      / cleaned['OrderId'].count().sum()*100),2)\n",
    "\n",
    "# The % of all global income (order_totals) placed by a country. \n",
    "df_countries['PctOf-Income'] = round((cleaned.groupby('ShipCountry')['order_total'].sum()\n",
    "                                    / cleaned['order_total'].sum()*100),2)\n",
    "\n",
    "#Checking results, Three countries had no disc.\n",
    "print(df_countries['PctOf-orders'].sum(),\n",
    "      df_countries['PctOf-Income'].sum()) \n",
    "\n",
    "\n",
    "#comparing percentage of all orders vs percentage on overall income \n",
    "df_countries['orders2-Income'] = (df_countries['PctOf-orders'] \n",
    "                                / df_countries['PctOf-Income'])\n",
    "#finding difference between full price and disc price\n",
    "df_countries['DiscountGap'] = (df_countries['AvgPrice_Full']\n",
    "                             - df_countries['AvgPrice_Disc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_countries.round(2).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stats to df\n",
    "# Mean\n",
    "# Std. Error Mean\n",
    "# Std. deviation\n",
    "df_header = df_countries.columns\n",
    "\n",
    "country_col_means = pd.Series(df_countries.mean(axis=0)) \n",
    "country_col_sem = pd.Series(df_countries.sem(axis=0))   \n",
    "country_col_std = pd.Series(df_countries.std(axis=0)) \n",
    "\n",
    "df_countries_means = pd.DataFrame([country_col_means, country_col_sem,\n",
    "                        country_col_std],columns=df_header)\n",
    "df_countries_means.index = ['mean','sem','std']\n",
    "df_countries_means.columns=df_countries.columns\n",
    "df_countries_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output if save_for_user == True \n",
    "if save_for_user==True:\n",
    "    export_data(df_countries,'table_H4_calc_features_by_country')\n",
    "    export_data(df_countries_means,'table_H4_calc_features_by_country_col_means_sem')\n",
    "\n",
    "# Concatenating df_country and mean sem dataframes. \n",
    "# df_countries = pd.concat([df_countries, df_countries_means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping countries based on critical value criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different criteria for exploring and anaqlyzing the current df\n",
    "# Used crit3 for determining the samples used in hypothesis 4\n",
    "\n",
    "# .433 is the mean value for Quant_Disc/Total\n",
    "crit3 = (df_countries['Quant_Disc/Total'] > df_countries_means.loc['mean','Quant_Disc/Total']) \n",
    "\n",
    "high_disc_countries = df_countries.loc[crit3]\n",
    "low_disc_countries = df_countries.loc[~crit3]\n",
    "high_disc_countries.index\n",
    "# Should return: \n",
    "# 'France', 'Germany', 'Brazil', 'Switzerland', 'Austria', 'USA','Sweden', 'Italy', 'Ireland', 'Portugal', 'Canada'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_disc_countries.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCQz20HQ7-oY"
   },
   "source": [
    "### ***H4, Aim 1.3: Using EDA to decide best metric for hypothesis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "with plt.style.context('ggplot'):\n",
    "    for col in high_disc_countries.columns:\n",
    "        fig = plt.figure()\n",
    "\n",
    "        plt.bar(x=high_disc_countries.index,\n",
    "                height=high_disc_countries[col],\n",
    "                yerr=df_countries_means.loc['sem',col])\n",
    "\n",
    "        ax = fig.gca()\n",
    "        ax.set_xticklabels(high_disc_countries.index, rotation=75)\n",
    "        ax.set_ylabel(col)\n",
    "        ax.axhline(y=0,color='w',ls=':',lw=1)\n",
    "        plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 4: Final Selected Hypothesis: \n",
    "> Do countries that buy discounted items in higher-than-average quantities spend more money (have higher order totals)?\n",
    "\n",
    "\n",
    "- $H_1$: Orders shipped to countries where average orders contain higher than average amounts of discounted items per order spend more per order on average than other countries.\n",
    "\n",
    "- $H_0$: Orders shipped to countries where average orders contain higher than average amounts of discounted items per order spend the same per order on average than other countries.\n",
    "\n",
    "\n",
    "  - I am defining countries with higher than average discount prices as the following:\n",
    "   Brazil, Belgium, Austria', Mexico, USA, Sweden, Spain, Ireland, Canada, Denmark.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df.drop_duplicates('OrderId')\n",
    "df_c.set_index('OrderId',inplace=True)\n",
    "# df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add three more features for easy analysis,\n",
    "- column 'Abovecrit' \n",
    "    - Boolean value for all countries whose discounted items/total order, is above average for all countries\n",
    "- column 'AbovecritGroups'\n",
    "    - subset 'Abovecrit' into 0 for all countries below critical value, and a unique categorical number for each country above.\n",
    "- column 'CountryCode'\n",
    "    - a unique code for each country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding all countries for comparison in a Tukeys test\n",
    "# Anything With a 1-12 met Criteria in crit3\n",
    "country_codes = {\n",
    "    'France':1,'Germany':2,\n",
    "    'Brazil':3, 'Belgium':13, \n",
    "    'Switzerland':4,'Venezuela':14,\n",
    "    'Austria':5,'Mexico':15,\n",
    "    'USA':6,'Sweden':7,\n",
    "    'Finland':16,'Italy':8,\n",
    "    'Spain':9,'UK':17,\n",
    "    'Ireland':10,'Portugal':11,\n",
    "    'Canada':12,'Denmark':18,\n",
    "    'Poland':19,'Norway':20,\n",
    "    'Argentina':21\n",
    "}\n",
    "# if save_for_user ==True:\n",
    "#     pd.country_codes.to_csv()\n",
    "#Unique codes for each country \n",
    "df_c.loc[:,'CountryCode'] = df_c['ShipCountry'].map(country_codes)\n",
    "\n",
    "#Boolean for two groups, Above crit or below\n",
    "df_c.loc[df_c['CountryCode'] <= 12, 'Abovecrit'] = True\n",
    "df_c.loc[df_c['CountryCode'] > 12, 'Abovecrit'] = False\n",
    "\n",
    "#Groups each country above criteria as its own group, rest of pop. isin its own group.\n",
    "df_c.loc[df_c['CountryCode'] > 12, 'AbovecritGroups'] = 0\n",
    "df_c.loc[df_c['CountryCode'] <= 12, 'AbovecritGroups'] = df_c['CountryCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define High Discount and Low Discount Order Lists\n",
    "- Re-named the series to analyze\n",
    "    - H4_sample --> H4_high_disc\n",
    "    - H4_pop -->H4_low_disc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make series of 'order_totals' for testing against criteria\n",
    "h4_high_disc = df_c.groupby('Abovecrit')['order_total'].get_group(1)\n",
    "h4_low_disc = df_c.groupby('Abovecrit')['order_total'].get_group(0)\n",
    "\n",
    "print(f'High Discount Groups: {h4_low_disc.mean().round(2)}\\n Low Discount Group {h4_high_disc.mean().round(2)}')\n",
    "\n",
    "# h4_low_disc= pd.DataFrame(h4_low_disc)\n",
    "# h4_high_disc= pd.DataFrame(h4_high_disc)\n",
    "# (h4_high_disc.head(),\n",
    "# h4_low_disc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4_high_disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going back to the original df to test the hypthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correct series 'oder_total'\n",
    "h4_high_disc.name, h4_low_disc.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wide_kde_thin_bar(h4_high_disc,\n",
    "                       'High Discount Countries',\n",
    "                       h4_low_disc,\n",
    "                       'Low Discount Countries')\n",
    "\n",
    "if save_for_user==True:\n",
    "    plt.savefig(fig_filepath+'H4_High vs Low Discount Countries KDE bar.png',dpi=300)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(('dark_background')):\n",
    "    \n",
    "    sns.set_palette('Dark2')\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.title('Distribution of total order:\\n High Discount vs Low Discount Countries')\n",
    "\n",
    "    disc = h4_high_disc #['order_total']\n",
    "    full = h4_low_disc #['order_total']\n",
    "    label = 'Low Discount Group'\n",
    "    \n",
    "    plt.hist(full, alpha = 0.8, bins=30,label=label)\n",
    "    plt.hist(disc, alpha = 0.5, bins=30,label='High Discount Group')\n",
    "\n",
    "    # Adding annotations\n",
    "    meanD = round(np.mean(disc),3)\n",
    "    meanF = round(np.mean(full),3)\n",
    "    \n",
    "    plt.axvline(meanD, color='green',linestyle='--',label='Discounted Mean')\n",
    "    plt.text(meanD,190,f'Mean:{meanD}',rotation=270,fontweight='medium')\n",
    "    \n",
    "    plt.axvline(meanF,color='white',linestyle='-',label='Full Price Mean')\n",
    "    plt.text(meanF, 190, f'Mean:{meanF}',rotation=270,fontweight='medium')\n",
    "\n",
    "    plt.xlabel('order_total')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***H4, Aim 2 : Test for assumptions of normality and equal variance, then choose test***\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H4_tests = [['Group', 'TestName','Test Purpose','stat','p','p < .05']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arrA and arrB for the following series of normality and equal variance tests\n",
    "arrA_name = 'High Discount Countries'\n",
    "arrA = h4_high_disc #.order_total\n",
    "arrB_name = 'Low Discount Countries'\n",
    "arrB = h4_low_disc #.order_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import normaltest\n",
    "# Test for normality\n",
    "\n",
    "test_purpose = 'Normality'\n",
    "test_to_run = 'normaltest'\n",
    "\n",
    "# arrA = h4_high_disc.order_total\n",
    "# arrB = h4_low_disc.order_total\n",
    "arrA = np.array(arrA)\n",
    "arrB = np.array(arrB)\n",
    "\n",
    "statA, pA = eval(test_to_run)(arrA)\n",
    "stat, p = eval(test_to_run)(arrB)\n",
    "\n",
    "# print(arrA_name, statA, pA,\n",
    "#       '\\n',arrB_name,stat,p)\n",
    "\n",
    "H4_tests.append([f'A: {arrA_name}', test_to_run, \n",
    "                 test_purpose , statA,\n",
    "                 pA, pA < .05])\n",
    "\n",
    "H4_tests.append([f'B: {arrB_name}', test_to_run, \n",
    "                 test_purpose, stat, p,\n",
    "                 p < .05])\n",
    "list2df(H4_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check variance \n",
    "- levenes test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "# Levenes test for testing homogeneity of variance\n",
    "# Choose this over Bartlett's due to  dev. from normality\n",
    "# Set to center='median' (default) because of skewed dist,\n",
    "# Use 'mean' for symetrical distribution\n",
    "# Use trimmed for heavy tailed distributions\n",
    "\n",
    "# arrA = h4_high_disc.order_total\n",
    "# arrB =h4_low_disc.order_total\n",
    "\n",
    "test_to_run = 'levene'\n",
    "test_purpose = 'Equal Variance'\n",
    "\n",
    "arrA = np.array(arrA)\n",
    "arrB = np.array(arrB)\n",
    "\n",
    "stat, p = eval(test_to_run)(arrA,arrB,\n",
    "                            center='median')\n",
    "print('A & B:', stat,p)\n",
    "\n",
    "H4_tests.append(['A & B:', test_to_run,\n",
    "                 test_purpose ,stat, p, p < .05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2df(H4_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table of basic info on central tendency\n",
    "from scipy.stats import sem\n",
    "H4_mean_sem = [['H#','Group','mean','sem','std','n']]\n",
    "\n",
    "dataArr=arrA\n",
    "H4_mean_sem.append(['H4',f'A: {arrA_name}',np.mean(dataArr),\n",
    "                    sem(dataArr), np.std(dataArr),len(dataArr)])\n",
    "\n",
    "dataArr=arrB\n",
    "H4_mean_sem.append(['H4',f'B: {arrB_name}',np.mean(dataArr),\n",
    "                    sem(dataArr), np.std(dataArr),len(dataArr)])\n",
    "\n",
    "table_H4_mean_sem = list2df(H4_mean_sem)\n",
    "\n",
    "# Export csv if user sets save_for_user to True\n",
    "if save_for_user==True:\n",
    "    export_data(table_H4_mean_sem,'table_H4_mean_sem')\n",
    "#     table_H1_mean_sem.to_csv(data_filepath+'table_H1_mean_sem.csv')    \n",
    "#     table_H1_mean_sem.to_excel(data_filepath+'table_H1_mean_sem.xlsx')    \n",
    "#     table_H1_mean_sem.round(2).to_html(data_filepath+'table_H1_mean_sem.html')    \n",
    "\n",
    "table_H4_mean_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test\n",
    "- Both sample and population are not normal and dont have equal variance.\n",
    "- ~~use Whitney-Mann U~~\n",
    "- use welch's t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Run a Welch's one sided t_test for unequal homogeneity\n",
    "# equal_var is set false, test compensates for this\n",
    "# Divide in half because it is a one sided test\n",
    "\n",
    "# arrA = h4_high_disc.order_total\n",
    "# arrB =h4_low_disc.order_total\n",
    "\n",
    "test_to_run = \"ttest_ind\"\n",
    "test_purpose = 'Hypothsis sig'\n",
    "\n",
    "arrA = np.array(arrA)\n",
    "arrB = np.array(arrB)\n",
    "\n",
    "stat, p = eval(test_to_run)(arrA,arrB,\n",
    "                            equal_var=False)\n",
    "p = p /2 \n",
    "# print('sample and pop:', stat, p)\n",
    "\n",
    "H4_tests.append(['A vs. B:', test_to_run,\n",
    "                 test_purpose ,stat, p, p < .05])                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mann-Whitney U test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the Mann Whitney U test\n",
    "# from scipy.stats import mannwhitneyu\n",
    "\n",
    "# # Mann Whitney U test for non normal distributioin\n",
    "# #Set alternative to 'greater' for one tail test\n",
    "\n",
    "# # arrA = h4_high_disc.order_total\n",
    "# # arrB =h4_low_disc.order_total\n",
    "\n",
    "# test_to_run = 'mannwhitneyu'\n",
    "# test_purpose = 'Hypothesis sig.'\n",
    "\n",
    "# arrA = np.array(arrA)\n",
    "# arrB = np.array(arrB)\n",
    "\n",
    "# stat, p = eval(test_to_run)(arrA,arrB,\n",
    "#                             alternative='greater')\n",
    "# print('A vs B:', stat, p)\n",
    "\n",
    "# H4_tests.append(['A vs B', test_to_run,\n",
    "#                  test_purpose ,stat, p, p < .05]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Effect Size\n",
    "d = Cohen_d(arrA,arrB)\n",
    "H4_tests.append(['A vs B:',\"Cohen's d\",'Effect Size',d,'--','--'])\n",
    "list2df(H4_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save a copy of the table H4 without the additional\n",
    "if save_for_user==True:\n",
    "    export_data(list2df(H4_tests),'table_H4_test_results')\n",
    "\n",
    "list2df(H4_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tukeys with subset of countries against the whole population\n",
    "# # Create labels\n",
    "# # Run Test\n",
    "# # Put results in df\n",
    "# # Checking for any signifigance\n",
    "# from statsmodels.stats.multicomp import pairwise_tukeyhsd as tukey\n",
    "\n",
    "# int_str = [str(x) for x in df_c['AbovecritGroups']]\n",
    "\n",
    "# tukey_results =tukey(df_c['order_total'],int_str, 0.05)\n",
    "# dfH_tukey = pd.DataFrame(data=tukey_results._results_table.data[1:],\n",
    "#                          columns=tukey_results._results_table.data[0])\n",
    "# dfH_tukey.loc[dfH_tukey['reject']==True]\n",
    "# # dfH_tukey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tukey's test with each country compared to each other country\n",
    "\n",
    "# int_str = [str(x) for x in df_c['CountryCode']]\n",
    "int_str = [str(x) for x in df_c['ShipCountry']]\n",
    "\n",
    "tukey_results = tukey(df_c['order_total'], int_str, 0.05)\n",
    "\n",
    "# Turn into a table with tukey_table\n",
    "dfH_tukey = tukey_table(tukey_results)\n",
    "# # Turn column names into title-capitalized \n",
    "# dfH_tukey.columns = [str(x).title() for x in dfH_tukey.columns]\n",
    "\n",
    "if save_for_user == True:\n",
    "    export_data(dfH_tukey,'table_H4_tukey_full')\n",
    "    export_data(dfH_tukey.loc[dfH_tukey['Reject']==True], 'table_H4_tukey_sig_only')\n",
    "\n",
    "dfH_tukey.loc[dfH_tukey['Reject']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(list2df(H4_tests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 4 Conclusions:\n",
    "- Our data had significant results for normality and equal variance tests. \n",
    "- We therefore ran a Mann Whitney on High Discount COuntries vs Low Discount Countries \n",
    "    - We had a significant difference and rejected the null hypothesis \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discounts spur consumer purchasing behavior. \n",
    "    - They buy discounted items in higher quantities. \n",
    "- They spend more money if they are purchasing 1+ discounted items.\n",
    "- Time of year (by month) was not significant, but further metrics worth investigating.\n",
    "- Customers from different countries have unique spending behaviors that may be capitalized on with further investigation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rzJx4u8wwQL"
   },
   "source": [
    "### Hypothesis 1:\n",
    "> **Do discounts have a statistically significant effect on the number of products customers order?**\n",
    "> **If so, at what level(s) of discount?**\n",
    "\n",
    "\n",
    "- $H_1$ : Products that are discounted sell in higher quantities.\n",
    "\n",
    "- $H_0$ : Products that are discounted sell the same quantities as full-price products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H1_mean_sem.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H1_test_results.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H1_tukey_sig_only.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rzJx4u8wwQL"
   },
   "source": [
    "### Hypothesis 2:\n",
    "> **Do customers spend more money if they are buying discounted items?**\n",
    "\n",
    "- $H_1$: Customers spend more money overall when their order includes discounted items.\n",
    "\n",
    "- $H_0$: Customers spend the same amount regardless of discounted items. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H2_mean_sem.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H2_test_results.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(filename=data_filepath+'table_H2_tukey.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rzJx4u8wwQL"
   },
   "source": [
    "### Hypothesis 3:\n",
    "\n",
    "> **Does the time of year affect quantity of items sold?**\n",
    "\n",
    "- $H_1$ = THe month an order is placed relates to either a higher or lower mean quantity of items sold.\n",
    "- $H_0$ = The month of an order has no affect on the mean quantity of items sold.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H3_mean_sem.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H3_test_results.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=data_filepath+'table_H3_tukey_sig_only.html')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "_Mod2_Project_JMI_04-07-19_H1-H3.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python36964bitlearnenvconda5e778df2e0df4e31951cf19db85f0750"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.297px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 525.1999999999999,
   "position": {
    "height": "40px",
    "left": "913px",
    "right": "20px",
    "top": "-15px",
    "width": "759.188px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
