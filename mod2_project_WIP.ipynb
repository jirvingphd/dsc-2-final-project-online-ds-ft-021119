{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Module 2 Final Project Specifications: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this lesson, we'll review all the guidelines and specifications for the final project for Module 2. \n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Understand all required aspects of the Final Project for Module 2\n",
    "* Understand all required deliverables\n",
    "* Understand what constitutes a successful project\n",
    "\n",
    "### Final Project Summary\n",
    "\n",
    "Another module down--you're half way there!\n",
    "\n",
    "< img src='halfway-there.gif'>\n",
    "\n",
    "For the culmination of Module 2, you just need to complete the final project!\n",
    "\n",
    "### The Project\n",
    "\n",
    "For this project, you'll be working with the Northwind database--a free, open-source dataset created by Microsoft containing data from a fictional company. You probably remember the Northwind database from our section on Advanced SQL. Here's the schema for the Northwind database:\n",
    "\n",
    "<img src='Northwind_ERD.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Deliverables\n",
    "> **The goal of this project** is to test your ability to gather information from a real-world database and use your knowledge of statistical analysis and hypothesis testing to generate analytical insights that can be of value to the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In addition to answering this question with a hypothesis test, you will also need to come up with **_at least 3 other hypotheses to test on your own_**.  \n",
    "\n",
    "These can by **anything that you think could be imporant information _for the company_.** \n",
    "\n",
    "For this hypothesis, be sure to specify both the **_null hypothesis_** and the **_alternative hypothesis_** for your question.  You should also specify if this is one-tail or a two-tail test. \n",
    "\n",
    "To complete this project, you will need to turn in the following 3 deliverables:\n",
    "\n",
    "1. A **_Jupyter Notebook_** containing any code you've written for this project. \n",
    "2. A **_Blog Post_** explaining your process, methodology, and findings.  \n",
    "3. An **_\"Executive Summary\" PowerPoint Presentation_** that explains the hypothesis tests you ran, your findings, and their relevance to company stakeholders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Jupyter Notebook Must-Haves\n",
    "\n",
    "For this project, your jupyter notebook should meet the following specifications:\n",
    "\n",
    "**_Organization/Code Cleanliness_**\n",
    "\n",
    "* The notebook should be well organized, easy to follow, and code is commented where appropriate.  \n",
    "<br>  \n",
    "    * Level Up: The notebook contains well-formatted, professional looking markdown cells explaining any substantial code. All functions have docstrings that act as professional-quality documentation.  \n",
    "<br>      \n",
    "* The notebook is written to technical audiences with a way to both understand your approach and reproduce your results. The target audience for this deliverable is other data scientists looking to validate your findings.  \n",
    "<br>    \n",
    "* Any SQL code written to source data should also be included.  \n",
    "\n",
    "**_Findings_**\n",
    "\n",
    "* Your notebook should clearly show how you arrived at your results for each hypothesis test, including how you calculated your p-values.   \n",
    "<br>\n",
    "* You should also include any other statistics that you find relevant to your analysis, such as effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Executive Summary Must-Haves\n",
    "\n",
    "Your presentation should:\n",
    "\n",
    "* Contain between 5-10 professional quality slides detailing:\n",
    "<br>  \n",
    "    * A high-level overview of your methodology  \n",
    "    <br>  \n",
    "    * The results of your hypothesis tests  \n",
    "    <br>  \n",
    "    * Any real-world recommendations you would like to make based on your findings (ask yourself--why should the executive team care about what you found? How can your findings help the company?)  \n",
    "    <br>  \n",
    "* Take no more than 5 minutes to present  \n",
    "<br>  \n",
    "* Avoid technical jargon and explain results in a clear, actionable way for non-technical audiences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Blog Post Must-Haves\n",
    "\n",
    "Your blog post should include everything from how you identified what tables contained the information you need, to how you retrieved it using SQL (and any challenges you ran into while doing so), as well as your methodology and results for your hypothesis tests. \n",
    "\n",
    "**_NOTE:_**  This blog post is your way of showcasing the work you've done on this project--chances are it will soon be read by a recruiter or hiring manager! Take the time to make sure that you craft your story well, and clearly explain your process and findings in a way that clearly shows both your technical expertise **_and_** your ability to communicate your results!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of Data Processing and Analysis<br> (using OSEMN model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **OBTAIN:**\n",
    "    - **Import data, inspect, check for datatypes to convert and null values**<br>\n",
    "        - Display header and info\n",
    "        - Drop any unneeded columns (df.drop(['col1','col2'],axis=1)\n",
    "\n",
    "2. **SCRUB: cast data types, identify outliers, check for multicollinearity, normalize data**<br>\n",
    "    - Check and cast data types\n",
    "        - [ ] Check for #'s that are store as objects (df.info())\n",
    "            - when converting to #'s, look for odd values (like many 0's), or strings that can't be converted\n",
    "            - Decide how to deal weird/null values (df.unique(), df.isna().sum(), df.describe()-min/max, etc\n",
    "        - [ ]  Check for categorical variables stored as integers\n",
    "    - [ ] Check for missing values  (df.isna().sum())\n",
    "        - Can drop rows or colums\n",
    "        - For missing numeric data with median or bin/convert to categorical\n",
    "        - For missing categorical data: make NaN own category OR replace with most common category\n",
    "    - [ ] Check for multicollinearity\n",
    "         - use seaborn to make correlation matrix plot [Evernote Link](https://www.evernote.com/l/AArNyaEwjA5JUL6I9PazHs_ts_hU-m7ja1I/) \n",
    "        - Good rule of thumb is anything over 0.75 corr is high, remove the variable that has the most correl with the largest # of variables\n",
    "    - [ ] Normalize data (may want to do after some exploring)\n",
    "        - Most popular is Z-scoring (but won't fix skew) \n",
    "        - Can log-transform to fix skewed data\n",
    "    \n",
    "            \n",
    "3. **EXPLORE:Check distributions, outliers, etc**\n",
    "    - [ ] Check scales, ranges (df.describe())\n",
    "    - [ ] Check histograms to get an idea of distributions (df.hist()) and dat transformations to perform\n",
    "        - Can also do kernel density estimates\n",
    "    - [ ] Use scatterplots to check for linearity and possible categorical variables (df.plot(kind-'scatter')\n",
    "        - categoricals will look like vertical lines\n",
    "    - [ ] Use pd.plotting.scatter_matrix to visualize possible relationships\n",
    "    - [ ] Check for linearity\n",
    "\n",
    "   \n",
    "4. **FIT AN INITIAL MODEL:** \n",
    "    - Various forms, detail later...\n",
    "    - **Assessing the model:**\n",
    "        - Assess parameters (slope,intercept)\n",
    "        - Check if the model explains the variation in the data (RMSE, F, R_square)\n",
    "        - *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "        - *Whats the impact of collinearity? Can we ignore?*\n",
    "5. **Revise the fitted model**\n",
    "    - Multicollinearity is big issue for lin regression and cannot fully remove it\n",
    "    - Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "    - Check for missed non-linearity\n",
    "6. **Holdout validation / Train/test split**\n",
    "    - use sklearn train_test_split \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL DESIGN PLANNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "You will need query the database to get the data needed to perform a statistical analysis.  In this statistical analysis, **you'll need to perform a hypothesis test (or perhaps several) to answer the following question:**\n",
    "\n",
    "\n",
    "> **_Do discounts have a statistically significant effect on the number of products customers order? If so, at what level(s) of discount?_**\n",
    "\n",
    "### Hypothesis 1: (Given Hypothesis)\n",
    "\n",
    "- $H_1$: The greater the discount, the greater the quantity of products ordered by individual customers?\n",
    "- $H_0$:  Discounts have no effect on the quantity of product ordered by individual customers.  \n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - **Aim 1:To select dataset that allows us to compare our target variable, **quantity _per purchase_**, vs the predictor variable discount.**\n",
    "    \n",
    "       - [ ] SELECT OrderID, QUantity,Discount FROM OrderDetails\n",
    "        - GROUPBY discount\n",
    "        - id = OrderId\\\n",
    "        \n",
    "    - **Aim 2: To test the relationship using a[ ]-tailed t-test.**\n",
    "        - One or two tailed?\n",
    "            - __ tailed, because...\n",
    "    - **Aim 3: To determine which level of discounts affect quantity?**\n",
    "    \n",
    " \n",
    "<br>\n",
    "\n",
    "### Hypothesis 2:\n",
    "\n",
    "- $H_1$: Customers buy more of other store items when they are buying discounted items.\n",
    "- $H_0$: Customers buy the same ***amount*** of items\n",
    "    - **amount could mean:**\n",
    "        - the # of items (discounted vs not discounted vs same level of discount?)\n",
    "        - total price per order with/without discounted products\n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - Aim 1:To select dataset that allows us to compare our target variable [    ] vs the predictor variable [      ].\n",
    "    - Aim 2: To test the relationship using a [ ] -tailed t-test. \n",
    "    - Aim 3: To determine which levels of the predictor underly the effect.\n",
    "\n",
    "### Hypothesis 3:\n",
    "\n",
    "- $H_1$: \n",
    "- $H_0$:   \n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - Aim 1:To select dataset that allows us to compare our target variable [    ] vs the predictor variable [      ].\n",
    "    - Aim 2: To test the relationship using a [ ] -tailed t-test. \n",
    "    - Aim 3: To determine which levels of the predictor underly the effect.\n",
    "    \n",
    "### Hypothesis 4:\n",
    "- $H_1$: \n",
    "- $H_0$:   \n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - Aim 1:To select dataset that allows us to compare our target variable [    ] vs the predictor variable [      ].\n",
    "    - Aim 2: To test the relationship using a [ ] -tailed t-test. \n",
    "    - Aim 3: To determine which levels of the predictor underly the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZATION AND DECLARATION OF FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normal packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "# Statsmodels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session, sessionmaker\n",
    "from sqlalchemy import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions (from proj 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### def check_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check columns returns the datatype, null values and unique values of input series \n",
    "def check_column(series,nlargest='all'):\n",
    "    print(f\"Column: df['{series.name}']':\")\n",
    "    print(f\"dtype: {series.dtype}\")\n",
    "    print(f\"isna: {series.isna().sum()} out of {len(series)} - {round(series.isna().sum()/len(series)*100,3)}%\")\n",
    "        \n",
    "    print(f'\\nUnique non-na values:') #,df['waterfront'].unique())\n",
    "    if nlargest =='all':\n",
    "        print(series.value_counts())\n",
    "    else:\n",
    "        print(series.value_counts().nlargest(nlargest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### def multiplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# MULTIPLOT\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def multiplot(df):\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, center=0,\n",
    "                \n",
    "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### def detect_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tukey's method using IQR to eliminate \n",
    "def detect_outliers(df,n,features):\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        # select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "        return multiple_outliers \n",
    "# Outliers_to_drop = detect_outliers(data,2,[\"col1\",\"col2\"])\n",
    "# df.loc[Outliers_to_drop] # Show the outliers rows\n",
    "# Drop outliers\n",
    "# data= data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def plot_hist_scat_sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEABORN\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots histogram and scatter (vs price) side by side\n",
    "def plot_hist_scat_sns(df,target='price'):\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    figsize=(10,6)\n",
    "    \n",
    "    ## ----------- DEFINE AESTHETIC CUSTOMIZATIONS ----------- ##\n",
    "    # Axis Label fonts\n",
    "    fontTitle = {'fontsize': 12,\n",
    "               'fontweight': 'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    fontAxis = {'fontsize': 10,\n",
    "               'fontweight': 'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    fontTicks = {'fontsize': 8,\n",
    "               'fontweight':'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    # Formatting dollar sign labels\n",
    "    fmtPrice = '${x:,.0f}'\n",
    "    tickPrice = mtick.StrMethodFormatter(fmtPrice)\n",
    "    \n",
    "\n",
    "    ## ----------- PLOTTING ----------- ##\n",
    "    \n",
    "    ## Loop through dataframe to plot\n",
    "    for column in df.describe():\n",
    "    \n",
    "        # Create figure with subplots for current column\n",
    "        # Note: in order to use identical syntax for large # of subplots (ax[i,j]), \n",
    "        #  declare an extra row of subplots to be removed later\n",
    "        fig, ax = plt.subplots(figsize=figsize, ncols=2, nrows=2)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## ----- SUBPLOT 1 -----##\n",
    "        i,j = 0,0\n",
    "        ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "        \n",
    "        # Define graphing keyword dictionaries for distplot (Subplot 1)\n",
    "        hist_kws = {\"linewidth\": 1, \"alpha\": 1, \"color\": 'blue','edgecolor':'w'}\n",
    "        kde_kws = {\"color\": \"white\", \"linewidth\": 1, \"label\": \"KDE\"}\n",
    "        \n",
    "        # Plot distplot on ax[i,j] using hist_kws and kde_kws\n",
    "        sns.distplot(df[column], norm_hist=True, kde=True,\n",
    "                     hist_kws = hist_kws, kde_kws = kde_kws,\n",
    "                     label=column+' histogram', ax=ax[i,j])\n",
    " \n",
    "\n",
    "        # Set x axis label\n",
    "        ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "    \n",
    "        # Get x-ticks, rotate labels, and return\n",
    "        xticklab1 = ax[i,j].get_xticklabels(which = 'both')\n",
    "        ax[i,j].set_xticklabels(labels=xticklab1, fontdict=fontTicks, rotation=45)\n",
    "        ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "        \n",
    "        # Set y-label \n",
    "        ax[i,j].set_ylabel('Density',fontdict=fontAxis)\n",
    "        yticklab1=ax[i,j].get_yticklabels(which='both')\n",
    "        ax[i,j].set_yticklabels(labels=yticklab1,fontdict=fontTicks)\n",
    "        ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "        \n",
    "        \n",
    "        # Set y-grid\n",
    "        ax[i, j].set_axisbelow(True)\n",
    "        ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ## ----- SUBPLOT 2-----  ##\n",
    "        i,j = 0,1\n",
    "        ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "        # Define the ketword dictionaries for  scatter plot and regression line (subplot 2)\n",
    "        line_kws={\"color\":\"white\",\"alpha\":0.5,\"lw\":4,\"ls\":\":\"}\n",
    "        scatter_kws={'s': 2, 'alpha': 0.5,'marker':'.','color':'blue'}\n",
    "\n",
    "        # Plot regplot on ax[i,j] using line_kws and scatter_kws\n",
    "        sns.regplot(df[column], df[target], \n",
    "                    line_kws = line_kws,\n",
    "                    scatter_kws = scatter_kws,\n",
    "                    ax=ax[i,j])\n",
    "        \n",
    "        # Set x-axis label\n",
    "        ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "         # Get x ticks, rotate labels, and return\n",
    "        xticklab2=ax[i,j].get_xticklabels(which='both')\n",
    "        ax[i,j].set_xticklabels(labels=xticklab2,fontdict=fontTicks, rotation=45)\n",
    "        ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "        # Set  y-axis label\n",
    "        ax[i,j].set_ylabel(target,fontdict=fontAxis)\n",
    "        \n",
    "        # Get, set, and format y-axis Price labels\n",
    "        yticklab = ax[i,j].get_yticklabels()\n",
    "        ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "        ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "#         ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "        # Set y-grid\n",
    "        ax[i, j].set_axisbelow(True)\n",
    "        ax[i, j].grid(axis='y',ls='--')       \n",
    "        \n",
    "        ## ---------- Final layout adjustments ----------- ##\n",
    "        # Deleted unused subplots \n",
    "        fig.delaxes(ax[1,1])\n",
    "        fig.delaxes(ax[1,0])\n",
    "\n",
    "        # Optimizing spatial layout\n",
    "        fig.tight_layout()\n",
    "        figtitle=column+'_dist_regr_plots.png'\n",
    "        plt.savefig(figtitle)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions (Proj 2 specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def list2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2df(list):#, sort_values='index'):\n",
    "    \"\"\" Take in a list where row[0] = column_names and outputs a dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    set_index -- df.set_index(set_index)\n",
    "    sortby -- df.sorted()\n",
    "    \"\"\"    \n",
    "    \n",
    "    df_list = pd.DataFrame(list[1:],columns=list[0])\n",
    "    df_list = df_list[1:]\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def get_col_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_col_info(col_name):\n",
    "    \n",
    "    col_list = inspector.get_columns(col_name)\n",
    "    \n",
    "    column_info = [['table','column','dtype']]\n",
    "    print(f'Table Name: {col_name}\\n')\n",
    "\n",
    "    for col in col_list:\n",
    "        column_info.append([str(col_name),col['name'], col['type']])\n",
    "        \n",
    "    df = list2df(column_info)\n",
    "    return column_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def get_full_table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_full_table_info(engine):\n",
    "    \n",
    "    column_info = [['table','column','dtype']]\n",
    "    \n",
    "    list_tables= engine.table_names()\n",
    "    \n",
    "    for table in list_tables:\n",
    "        \n",
    "        col_list = inspector.get_columns(table)\n",
    "        \n",
    "        for col in col_list:\n",
    "            \n",
    "            column_info.append([str(table),col['name'], col['type'],col['']])\n",
    "            inspector.get_foreign_keys()\n",
    "    \n",
    "    df = list2df(column_info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Method \n",
    "- Use sqlalchemy to create engine to connect to Northwind_small.sqlite.\n",
    "- use pd.read_sql_query('SELECT * FROM OrderDetail',egine) to directly read db into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-31 00:23:54,100 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n",
      "2019-03-31 00:23:54,105 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-03-31 00:23:54,108 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n",
      "2019-03-31 00:23:54,110 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-03-31 00:23:54,114 INFO sqlalchemy.engine.base.Engine SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\n",
      "2019-03-31 00:23:54,115 INFO sqlalchemy.engine.base.Engine ()\n",
      "\n",
      " ['Category', 'Customer', 'CustomerCustomerDemo', 'CustomerDemographic', 'Employee', 'EmployeeTerritory', 'Order', 'OrderDetail', 'Product', 'Region', 'Shipper', 'Supplier', 'Territory']\n"
     ]
    }
   ],
   "source": [
    "# Testing minimal version of prior code\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "\n",
    "engine = create_engine(\"sqlite:///Northwind_small.sqlite\",echo=True)\n",
    "inspector = inspect(engine);\n",
    "db_tables = inspector.get_table_names()\n",
    "print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-31 00:23:54,131 INFO sqlalchemy.engine.base.Engine PRAGMA table_info(\"Order\")\n",
      "2019-03-31 00:23:54,133 INFO sqlalchemy.engine.base.Engine ()\n",
      "Table Name: Order\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Order</td>\n",
       "      <td>CustomerId</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Order</td>\n",
       "      <td>EmployeeId</td>\n",
       "      <td>INTEGER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Order</td>\n",
       "      <td>OrderDate</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Order</td>\n",
       "      <td>RequiredDate</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShippedDate</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipVia</td>\n",
       "      <td>INTEGER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Order</td>\n",
       "      <td>Freight</td>\n",
       "      <td>DECIMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipName</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipAddress</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipCity</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipRegion</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipPostalCode</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Order</td>\n",
       "      <td>ShipCountry</td>\n",
       "      <td>VARCHAR(8000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    table          column          dtype\n",
       "1   Order      CustomerId  VARCHAR(8000)\n",
       "2   Order      EmployeeId        INTEGER\n",
       "3   Order       OrderDate  VARCHAR(8000)\n",
       "4   Order    RequiredDate  VARCHAR(8000)\n",
       "5   Order     ShippedDate  VARCHAR(8000)\n",
       "6   Order         ShipVia        INTEGER\n",
       "7   Order         Freight        DECIMAL\n",
       "8   Order        ShipName  VARCHAR(8000)\n",
       "9   Order     ShipAddress  VARCHAR(8000)\n",
       "10  Order        ShipCity  VARCHAR(8000)\n",
       "11  Order      ShipRegion  VARCHAR(8000)\n",
       "12  Order  ShipPostalCode  VARCHAR(8000)\n",
       "13  Order     ShipCountry  VARCHAR(8000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2df(get_col_info('Order'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-31 00:23:54,184 INFO sqlalchemy.engine.base.Engine SELECT * FROM OrderDetail\n",
      "2019-03-31 00:23:54,186 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10248/11</td>\n",
       "      <td>10248</td>\n",
       "      <td>11</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10248/42</td>\n",
       "      <td>10248</td>\n",
       "      <td>42</td>\n",
       "      <td>9.8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10248/72</td>\n",
       "      <td>10248</td>\n",
       "      <td>72</td>\n",
       "      <td>34.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10249/14</td>\n",
       "      <td>10249</td>\n",
       "      <td>14</td>\n",
       "      <td>18.6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10249/51</td>\n",
       "      <td>10249</td>\n",
       "      <td>51</td>\n",
       "      <td>42.4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  OrderId  ProductId  UnitPrice  Quantity  Discount\n",
       "0  10248/11    10248         11       14.0        12       0.0\n",
       "1  10248/42    10248         42        9.8        10       0.0\n",
       "2  10248/72    10248         72       34.8         5       0.0\n",
       "3  10249/14    10249         14       18.6         9       0.0\n",
       "4  10249/51    10249         51       42.4        40       0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to test hypothesis one, so we need OrderDetail table.\n",
    "table_to_test = \"OrderDetail\"\n",
    "df = pd.read_sql_query(\"SELECT * FROM OrderDetail\",  engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2155 entries, 0 to 2154\n",
      "Data columns (total 6 columns):\n",
      "Id           2155 non-null object\n",
      "OrderId      2155 non-null int64\n",
      "ProductId    2155 non-null int64\n",
      "UnitPrice    2155 non-null float64\n",
      "Quantity     2155 non-null int64\n",
      "Discount     2155 non-null float64\n",
      "dtypes: float64(2), int64(3), object(1)\n",
      "memory usage: 101.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @NOW<img src=\"https://www.dropbox.com/s/6xqzendi1iyzls8/bookmark.png?raw=1\" width=25>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%git` not found.\n"
     ]
    }
   ],
   "source": [
    "%git add .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c15e21d0dc02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheck_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Quantity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'check_column' is not defined"
     ]
    }
   ],
   "source": [
    "check_column(df['Quantity'],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine df stats by discount level\n",
    "df.groupby('Discount').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(df['Discount'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Note on OrderDetails df:\n",
    "1. There are 11 possible values for discounts: \n",
    "    - Values = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1, 0.15, 0.2, 0.25]<br>\n",
    "    ``` print(sorted(df['Discount'].unique())) ```\n",
    "2. Too many columns to see.\n",
    "    - Can I stack/unstack to see better?\n",
    "    \n",
    "### Now how to proceed?\n",
    "1. Check what I need to run a t-test in python. \n",
    "    - [ ] Once review, use info to determine next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_hist_scat_sns(df,'Quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT FIGURE OUT HOW TO EXAMINE THE KDE FOR ...EACH Discount level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show counts of discounts by qunatit\n",
    "# df.apply(pd.Series.value_counts)\n",
    "test = df.iloc[df.groupby(['Discount'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc_quant = df.groupby(['Discount','Quantity']).count() #.plot(kind='bar')\n",
    "df_disc_quant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing extracting info from groupby and describe\n",
    "# df_test = df.groupby('Discount')#.describe().stack()\n",
    "# df_cols = df_test.columns\n",
    "# df_cols\n",
    "# # # Turn df_cols into list \n",
    "# # df_cols=[x for x in df_cols]\n",
    "# # df_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### From following sqlalchemy tutorial slides, currently note used\n",
    "<a href=\"http://www.dropbox.com/s/rt6s9rqp06m2vky/sqlalchemy%20slides.pdf?dl=0\">Introduction to Alchemy Slides</a>\n",
    "```python\n",
    "metadata=MetaData()\n",
    "# Now use sqlalchemy\n",
    "order_table = Table('OrderDetail', metadata, \n",
    "                    Column('ProductId',Integer,primary_key=True),\n",
    "                    Column('Discount',Float),\n",
    "                    Column('OrderId',Integer),\n",
    "                    Column('Quantity',Integer))\n",
    "\n",
    "\n",
    "# order_table.create(engine)\n",
    "# metadata.create_all(engine)\n",
    "con = engine.connect()\n",
    "rs = con.execute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Import sqalchemy\n",
    "# import sqlalchemy\n",
    "# from sqlalchemy import create_engine, inspect\n",
    "# from sqlalchemy.orm import Session, sessionmaker\n",
    "# from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "\n",
    "# # Create engine to connect to database\n",
    "# engine = create_engine(\"sqlite:///Northwind_small.sqlite\",echo=True)\n",
    "# Session = sessionmaker(bind=engine)\n",
    "# session = Session()\n",
    "\n",
    "# # Inspect the table names of the database. \n",
    "# inspector = inspect(engine);\n",
    "# db_tables = inspector.get_table_names()\n",
    "# print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop\n",
    "df_tables = get_full_table_info(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables['table'].unique()\n",
    "df_tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 338.79999999999995,
   "position": {
    "height": "40px",
    "left": "868px",
    "right": "20px",
    "top": "70px",
    "width": "558.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
